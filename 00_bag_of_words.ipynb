{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "00_bag-of-words.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOKm79dWs0Nut9hBXSnwjAK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course-nlp2020/blob/master/00_bag_of_words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKEP5r0nOxjd",
        "colab_type": "text"
      },
      "source": [
        "### 軽い前置き\n",
        "* この授業では、Pythonのコーディングの基礎は習得済みであることを前提します。\n",
        "* また、NumPyやscikit-learnの基本的な使い方は習得済みであることを前提します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z57oKsOVGOWf",
        "colab_type": "text"
      },
      "source": [
        "# 00 bag-of-wordsモデル\n",
        "* **bag-of-wordsモデル**とは、文書をベクトルとしてモデル化する手法のひとつ。\n",
        " * 他にもそのような手法はある。\n",
        "* bag-of-wordsモデルにおいては、文書における単語トークンの**出現順序が無視される**。\n",
        " * 単語トークンとは、単語の一回一回の出現のこと。\n",
        "* つまり、文書を、バッグに入ったアイテムの集まりのように扱うモデリング（下図参照）。\n",
        " * 数学的に言えば、文書を単語の**multiset**として扱うのがbag-of-wordsモデル。\n",
        "\n",
        "* 参考資料\n",
        " * https://github.com/aws-samples/aws-machine-learning-university-accelerated-nlp/blob/master/notebooks/MLA-NLP-Lecture1-BOW.ipynb\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBk-WNX4IRVb",
        "colab_type": "text"
      },
      "source": [
        "![bag-of-words.png](https://raw.githubusercontent.com/tomonari-masada/course-nlp2020/master/bag-of-words.png)\n",
        "\n",
        "https://dudeperf3ct.github.io/lstm/gru/nlp/2019/01/28/Force-of-LSTM-and-GRU/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-5GrvauMKDq",
        "colab_type": "text"
      },
      "source": [
        "* 現在では、言語データをモデル化するとき、必ずと言っていいほど、単語（あるいはsubword）のベクトル表現を用いる。\n",
        "* この単語ベクトル表現は、埋め込みembeddingとも呼ばれるし、分散表現とも呼ばれる。\n",
        "* 単語埋め込みが急速に普及したのは、深層学習の普及とほぼ同時期。\n",
        "* それまではbag-of-wordsが広く使われていた。\n",
        "* 論文では今でも、文書分類のbaselineとして、TF-IDFやBM25など、bag-of-wordsモデルが引き合いに出されることがある。\n",
        " * 例えば、文書分類の新手法を考え出してもbag-of-wordsに勝てなければ意味がない、といった使い方をされる。\n",
        "* そのため、授業の最初のトピックとして、bag-of-wordモデルについて簡単に説明しておく。\n",
        " * BM25については右リンクを参照。 https://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Jn1BM6BJtei",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "興味がある方は、スタンフォード大の自然言語処理の授業が、この10年間でいかに大きく内容を変えているか、調べてみましょう。\n",
        "\n",
        "https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/index.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJZFJ9B0KNQO",
        "colab_type": "text"
      },
      "source": [
        "## 00-01 binary vector\n",
        "* 最も単純には、文書は、語彙に含まれる各単語が出現するかしないかの2値ベクトルでモデル化できる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uhf7MJe4NH3P",
        "colab_type": "text"
      },
      "source": [
        "### scikit-learnのCountVectorizer\n",
        "* 各documentは、半角スペースでつながれた単語の列として準備しておく。\n",
        " * これをどうやって作るかは、後で説明する。\n",
        "* binaryをTrueにセットすると、0/1の2値ベクトルが得られる。\n",
        "* インスタンスを作り、fit_transformする、という使い方は、scikit-learnにおけるデータの前処理のときと同様。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAsV0OzFGKr8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSPMo5HnOeLK",
        "colab_type": "text"
      },
      "source": [
        "### 文書の集合＝コーパスを用意する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JvB4acfMxQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = [\"This document is the first document.\",\n",
        "          \"This document is the second document.\",\n",
        "          \"And this is the third one.\",\n",
        "          \"Where is the fourth one?\"]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9piYSlbPSan",
        "colab_type": "text"
      },
      "source": [
        "### CountVectorizerをbinary=Trueで使う"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEcGpcc4M7bo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "binary_vectorizer = CountVectorizer(binary=True) # 2値ベクトルとして表現\n",
        "X = binary_vectorizer.fit_transform(corpus)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16VoXJLcNtmW",
        "colab_type": "text"
      },
      "source": [
        "### 文書の2値ベクトル表現の確認\n",
        "* 疎なベクトルとして得られることに注意。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Nl4fjykNDo7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "f27697fb-1e1c-48a3-cd74-52653f746f05"
      },
      "source": [
        "print(X)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 9)\t1\n",
            "  (0, 1)\t1\n",
            "  (0, 4)\t1\n",
            "  (0, 7)\t1\n",
            "  (0, 2)\t1\n",
            "  (1, 9)\t1\n",
            "  (1, 1)\t1\n",
            "  (1, 4)\t1\n",
            "  (1, 7)\t1\n",
            "  (1, 6)\t1\n",
            "  (2, 9)\t1\n",
            "  (2, 4)\t1\n",
            "  (2, 7)\t1\n",
            "  (2, 0)\t1\n",
            "  (2, 8)\t1\n",
            "  (2, 5)\t1\n",
            "  (3, 4)\t1\n",
            "  (3, 7)\t1\n",
            "  (3, 5)\t1\n",
            "  (3, 10)\t1\n",
            "  (3, 3)\t1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUICjJZ2N6px",
        "colab_type": "text"
      },
      "source": [
        "### 疎な表現を通常のndarrayに戻す"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJYPyDPDNsIL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "85ef5a74-b9d1-4569-d500-325541768b93"
      },
      "source": [
        "X.toarray()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0],\n",
              "       [0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0],\n",
              "       [1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0],\n",
              "       [0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzbZQPHuOAIP",
        "colab_type": "text"
      },
      "source": [
        "### 語彙を得る\n",
        "* 先頭の大文字は自動的に小文字に変換されていることが分かる。\n",
        "* ピリオドや疑問符は削除されている。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQAAI9LKN5vL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "1481d2ed-acce-4089-8e6c-d99a147e3044"
      },
      "source": [
        "binary_vectorizer.vocabulary_"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'and': 0,\n",
              " 'document': 1,\n",
              " 'first': 2,\n",
              " 'fourth': 3,\n",
              " 'is': 4,\n",
              " 'one': 5,\n",
              " 'second': 6,\n",
              " 'the': 7,\n",
              " 'third': 8,\n",
              " 'this': 9,\n",
              " 'where': 10}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P1dFVlcN_Pd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "03828d6c-a2ef-47c2-f45a-b1a0d9b274a6"
      },
      "source": [
        "print(binary_vectorizer.get_feature_names())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['and', 'document', 'first', 'fourth', 'is', 'one', 'second', 'the', 'third', 'this', 'where']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4AeNQK2P1GW",
        "colab_type": "text"
      },
      "source": [
        "### 新しい文書をベクトルに変換する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egmnVXnHPbkX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_doc = [\"This is the new document.\"]\n",
        "\n",
        "new_vectors = binary_vectorizer.transform(new_doc)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHWGA6W4Psto",
        "colab_type": "text"
      },
      "source": [
        "### 新出の単語は無視される点に注意"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqnzGOgmPj5B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "59f823c0-cc1a-4f6b-a6d0-99857726cdb4"
      },
      "source": [
        "new_vectors.toarray()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTPClzcmP84l",
        "colab_type": "text"
      },
      "source": [
        "## 00-02 word counts\n",
        "* 文書における各単語の出現回数を使って、文書のベクトル表現を得ることもできる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCBPOUUoQNoC",
        "colab_type": "text"
      },
      "source": [
        "### CountVectorizerをデフォルト設定で使う"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZNhmGTuPmo4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count_vectorizer = CountVectorizer()\n",
        "X = count_vectorizer.fit_transform(corpus)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yzMyTrhQWmW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "62e71dea-726e-48d9-c2eb-44b769c6e2f2"
      },
      "source": [
        "X.toarray()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 2, 1, 0, 1, 0, 0, 1, 0, 1, 0],\n",
              "       [0, 2, 0, 0, 1, 0, 1, 1, 0, 1, 0],\n",
              "       [1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0],\n",
              "       [0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE_YpgstQfL_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b8bbcfd6-1f22-485c-fa12-6d12710adab5"
      },
      "source": [
        "new_vectors = count_vectorizer.transform(new_doc)\n",
        "new_vectors.toarray()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7M88Lm1QwQF",
        "colab_type": "text"
      },
      "source": [
        "## 00-03 TF-IDF\n",
        "* TF-IDFは、TFとIDFの積である。\n",
        "* 文書に含まれる単語トークンの数（つまり、単語の出現回数の総和）を、その文書の長さと呼ぶ。\n",
        "* TFとは、各々の単語が文書のなかで出現する回数を、その文書の長さで割ったものである。\n",
        " * 文書のなかで頻出する単語ほどTFは大きくなる。\n",
        "* IDFとは、DFの逆数である。\n",
        "* DFとは、ある単語が含まれる文書の数を、総文書数で割ったものである。\n",
        " * 文書集合のなかで稀少な単語ほどIDFは大きくなる。\n",
        "\n",
        "* TF-IDFはTFとIDFの積であるが、積を求める前に、TFのルートを取ったり対数をとったり、あるいは、IDFのルートをとったり対数をとったりする。\n",
        " * 対数をとるときは、ゼロの対数をとることにならないような工夫をする。\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0RtY0PtSq3D",
        "colab_type": "text"
      },
      "source": [
        "### TF-IDFの式の例\n",
        "\n",
        "\\begin{align}\n",
        "x_{d,w} = \\frac{n_{d,w}}{n_d} \\cdot ( 1 + \\ln\\frac{m}{m_w}) \\tag{1}\n",
        "\\end{align}\n",
        "\n",
        "where \n",
        "\n",
        " * $n_{d,w}$ is the frequency of the word $w$ in the document $d$, \n",
        " * $n_d$ is defined as $n_d \\equiv \\sum_w n_{d,w}$,\n",
        " * $m_w$ is the number of documents containing the word $w$, and\n",
        " * $m$ is the total number of documents.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "* どの式の形がいいかは、downstream taskの性能をcross validationで評価して選ぶ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsLF9WdGHgFD",
        "colab_type": "text"
      },
      "source": [
        "![img462.png](https://raw.githubusercontent.com/tomonari-masada/course-nlp2020/master/img462.png)\n",
        "\n",
        "https://nlp.stanford.edu/IR-book/html/htmledition/document-and-query-weighting-schemes-1.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XfakOH8HqZ9",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9_0aZB0TOnl",
        "colab_type": "text"
      },
      "source": [
        "### scikit-learnのTfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KisBcoHQjn8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cA_F0D0tGMKZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "ecf40ea6-ff98-434a-b45a-0dfa2b29a9a2"
      },
      "source": [
        "# デフォルトの設定を確認してみる。\n",
        "# 次のパラメータは変更していいかもしれない。\n",
        "# max_df, min_df, ngram_range, norm, smooth_idf, stop_words, sublinear_tf\n",
        "tfidf_vectorizer"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
              "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
              "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
              "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
              "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, use_idf=True, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZYEcdYaTV28",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "f057e5ff-6b2e-473d-d96a-119f988d5d6b"
      },
      "source": [
        "X = tfidf_vectorizer.fit_transform(corpus).toarray()\n",
        "X"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.74846041, 0.47466356, 0.        , 0.24769914,\n",
              "        0.        , 0.        , 0.24769914, 0.        , 0.3029716 ,\n",
              "        0.        ],\n",
              "       [0.        , 0.74846041, 0.        , 0.        , 0.24769914,\n",
              "        0.        , 0.47466356, 0.24769914, 0.        , 0.3029716 ,\n",
              "        0.        ],\n",
              "       [0.52898651, 0.        , 0.        , 0.        , 0.2760471 ,\n",
              "        0.41705904, 0.        , 0.2760471 , 0.52898651, 0.33764523,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.56199026, 0.29326983,\n",
              "        0.44307958, 0.        , 0.29326983, 0.        , 0.        ,\n",
              "        0.56199026]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kSbn0uyUxOs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8266fd82-0431-4adf-ecb7-f56a06c20090"
      },
      "source": [
        "# 文書ベクトルはL2ノルムが1となるように長さを変更されている。（normパラメータで変更可能。）\n",
        "import numpy as np\n",
        "np.linalg.norm(X, axis=1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPGd8r-pIAVl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "d76241a3-0cb6-41dc-9151-de84e8efc256"
      },
      "source": [
        "tfidf_vectorizer.get_feature_names()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['and',\n",
              " 'document',\n",
              " 'first',\n",
              " 'fourth',\n",
              " 'is',\n",
              " 'one',\n",
              " 'second',\n",
              " 'the',\n",
              " 'third',\n",
              " 'this',\n",
              " 'where']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ-M-qZjTYsL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "0ede9345-b8b2-4826-ab81-e21b23b3f88c"
      },
      "source": [
        "new_vectors = tfidf_vectorizer.transform(new_doc).toarray()\n",
        "new_vectors"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.6284927 , 0.        , 0.        , 0.41599288,\n",
              "        0.        , 0.        , 0.41599288, 0.        , 0.50881901,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--XKc4eATjf_",
        "colab_type": "text"
      },
      "source": [
        "### 各単語のIDF\n",
        "* IDFはそれぞれの単語について一意に決まる値。\n",
        "* 文書ごとに求まる値ではない。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugsq3YAtTfRx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "4005ef98-b999-4b62-faa9-7c5e968b1cfc"
      },
      "source": [
        "tfidf_vectorizer.idf_"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.91629073, 1.51082562, 1.91629073, 1.91629073, 1.        ,\n",
              "       1.51082562, 1.91629073, 1.        , 1.91629073, 1.22314355,\n",
              "       1.91629073])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8niqN06T-eX",
        "colab_type": "text"
      },
      "source": [
        "### bag-of-wordsベクトルの使い方\n",
        "* 文書どうしの類似度の計算"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP0KvdykThdg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "2be82771-3f7c-46c4-c267-e0bba6345df4"
      },
      "source": [
        "# 長さ１にnormalizeされているので内積がコサイン類似度に一致する。\n",
        "for i in range(4):\n",
        "  print(np.dot(X[i], new_vectors[0]))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8306417662781155\n",
            "0.8306417662781155\n",
            "0.401467570331823\n",
            "0.24399632162751606\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}