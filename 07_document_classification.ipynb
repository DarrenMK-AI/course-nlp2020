{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07_document_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPaIoKZYtROwj0huBMOZ/3o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course-nlp2020/blob/master/07_document_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_0ZZ8bo1mgH",
        "colab_type": "text"
      },
      "source": [
        "# 07 文書分類\n",
        "* 今回は、fastTextのような学習済みの単語埋め込みは使わない。\n",
        "* 単語埋め込み自体の学習も、ネットワークの重みの学習と同時におこなう。\n",
        "* IMDbデータの準備も、PyTorchのtorchtextモジュールを使っておこなう。\n",
        "* ネットワークへの入力は、単語埋め込みを、単語の出現順どおりに並べた列にする。\n",
        "* そして、前向き計算のなかではじめて、単語埋め込みの平均をとることにする。\n",
        "* 参考資料\n",
        " * https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
        " * https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb\n",
        " * https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_puYg6Zi8x3",
        "colab_type": "text"
      },
      "source": [
        "## 07-00 下準備\n",
        "* Google ColabのランタイムのタイプをGPUに変更しておこう。\n",
        " * 上のメニューの「ランタイム」→「ランタイムのタイプを変更」→「ハードウェア　アクセラレータ」から「GPU」を選択"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLEeO0fw23Xp",
        "colab_type": "text"
      },
      "source": [
        "## 07-01 torchtextを使ってIMDbデータを読み込む\n",
        "* https://torchtext.readthedocs.io/en/latest/datasets.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go7epLZe3JmF",
        "colab_type": "text"
      },
      "source": [
        "### 実験の再現性確保のための設定など\n",
        "* torch.backends.cudnn.deterministicをTrueにするのは、こうしないと、GPU上での計算が毎回同じ値を与えないため。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nSqNzof1lTJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "\n",
        "SEED = 123\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y1_GyXg22f6",
        "colab_type": "text"
      },
      "source": [
        "### torchtextのフィールド\n",
        "* TEXTフィールドと、LABELフィールドという２種類のFieldオブジェクトのインスタンスを作る。\n",
        "* TEXTフィールドは、テキストの前処理の仕方を決めておくのに使う。\n",
        " * batch_firstをTrueに設定するのが今回のポイント。\n",
        " * tokenizerは、デフォルトでは単にstring型のsplitメソッドを適用するだけになる。これは高速だが、tokenizationとしては雑。\n",
        "* LABELフィールドは、ラベルの前処理に使う。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjq8oooE2uQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = data.Field(tokenize=\"spacy\", batch_first=True)\n",
        "LABEL = data.LabelField()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtEq23GS3Vxl",
        "colab_type": "text"
      },
      "source": [
        "### IMDbデータセットを前処理しつつ読み込む\n",
        "* TEXTフィールドでspaCyのtokenizationを使うように設定したので、少し時間がかかる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzgVXf3G3YPI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dev_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0sltPjT3j36",
        "colab_type": "text"
      },
      "source": [
        "### 最初の文書を見てみる\n",
        "* `vars`関数は、モジュール、クラス、インスタンス、あるいはそれ以外の`__dict__`属性を持つオブジェクトの、`__dict__`属性を辞書として返す組み込み関数。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "692vrq6B3gZe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "cc2bf01c-3a76-4fac-ae09-7fbde36933fa"
      },
      "source": [
        "print(vars(train_dev_data.examples[0]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': ['I', 'really', 'liked', 'this', 'movie', ',', 'and', 'went', 'back', 'to', 'see', 'it', 'two', 'times', 'more', 'within', 'a', 'week.<br', '/><br', '/>Ms', '.', 'Detmers', 'nailed', 'the', 'performance', '-', 'she', 'was', 'like', 'a', 'hungry', 'cat', 'on', 'the', 'prowl', ',', 'toying', 'with', 'her', 'prey', '.', 'She', 'lashes', 'out', 'in', 'rage', 'and', 'lust', ',', 'taking', 'a', '\"', 'too', 'young', '\"', 'lover', ',', 'and', 'crashing', 'hundreds', 'of', 'her', 'terrorist', 'fiancé', \"'s\", 'mother', \"'s\", 'pieces', 'of', 'fine', 'china', 'to', 'the', 'floor', '.', '<', 'br', '/><br', '/>The', 'film', 'was', 'full', 'of', 'beautiful', 'touches', '.', 'The', 'Maserati', ',', 'the', 'wonderful', 'wardrobe', ',', 'the', 'flower', 'boxes', 'along', 'the', 'rooftops', '.', 'I', 'particularly', 'enjoyed', 'the', 'ancient', 'Greek', 'class', 'and', 'the', 'recitation', 'of', \"'\", \"Antigone'.<br\", '/><br', '/>It', 'had', 'a', 'feeling', 'of', \"'\", 'Story', 'of', 'O', \"'\", '-', 'that', 'is', ',', 'where', 'people', 'of', 'means', 'indulge', 'in', 'unrestrained', 'sexual', 'adventure', '.', 'As', 'she', 'walks', 'around', 'the', 'fantastic', 'apartment', 'in', 'the', 'buff', ',', 'she', 'is', 'at', 'ease', '-', 'and', 'why', 'not', ',', 'what', 'is', 'to', 'restrain', 'a', '\"', 'Devil', 'in', 'the', 'Flesh\"?<br', '/><br', '/>The', 'whole', 'movie', 'is', 'a', 'real', 'treat', '!'], 'label': 'pos'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrXwYMVH3orf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5be727db-e515-44b7-af5c-cd2ee9ea5f4e"
      },
      "source": [
        "print(train_dev_data.examples[0].text)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'really', 'liked', 'this', 'movie', ',', 'and', 'went', 'back', 'to', 'see', 'it', 'two', 'times', 'more', 'within', 'a', 'week.<br', '/><br', '/>Ms', '.', 'Detmers', 'nailed', 'the', 'performance', '-', 'she', 'was', 'like', 'a', 'hungry', 'cat', 'on', 'the', 'prowl', ',', 'toying', 'with', 'her', 'prey', '.', 'She', 'lashes', 'out', 'in', 'rage', 'and', 'lust', ',', 'taking', 'a', '\"', 'too', 'young', '\"', 'lover', ',', 'and', 'crashing', 'hundreds', 'of', 'her', 'terrorist', 'fiancé', \"'s\", 'mother', \"'s\", 'pieces', 'of', 'fine', 'china', 'to', 'the', 'floor', '.', '<', 'br', '/><br', '/>The', 'film', 'was', 'full', 'of', 'beautiful', 'touches', '.', 'The', 'Maserati', ',', 'the', 'wonderful', 'wardrobe', ',', 'the', 'flower', 'boxes', 'along', 'the', 'rooftops', '.', 'I', 'particularly', 'enjoyed', 'the', 'ancient', 'Greek', 'class', 'and', 'the', 'recitation', 'of', \"'\", \"Antigone'.<br\", '/><br', '/>It', 'had', 'a', 'feeling', 'of', \"'\", 'Story', 'of', 'O', \"'\", '-', 'that', 'is', ',', 'where', 'people', 'of', 'means', 'indulge', 'in', 'unrestrained', 'sexual', 'adventure', '.', 'As', 'she', 'walks', 'around', 'the', 'fantastic', 'apartment', 'in', 'the', 'buff', ',', 'she', 'is', 'at', 'ease', '-', 'and', 'why', 'not', ',', 'what', 'is', 'to', 'restrain', 'a', '\"', 'Devil', 'in', 'the', 'Flesh\"?<br', '/><br', '/>The', 'whole', 'movie', 'is', 'a', 'real', 'treat', '!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRMuAOum3rB5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d4b35802-1d37-4b07-d638-dbda94242ee9"
      },
      "source": [
        "print(train_dev_data.examples[0].label)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgZgQbyD3u9D",
        "colab_type": "text"
      },
      "source": [
        "### テストセット以外の部分を訓練データと検証データに分ける"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2FtnEKZ32hM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, dev_data = train_dev_data.split(split_ratio=0.8, random_state = random.seed(SEED))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fzsi9ZC36eR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "f7c00e6e-2ef3-4b05-d024-0c40d67a213f"
      },
      "source": [
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of development examples: {len(dev_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 20000\n",
            "Number of development examples: 5000\n",
            "Number of testing examples: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oXz2lvB37Vm",
        "colab_type": "text"
      },
      "source": [
        "### データセットのラベルを作る\n",
        "* TEXTラベルのほうでは、最大語彙サイズを指定する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBQeD7yC37x4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_VOCAB_SIZE = 25000\n",
        "\n",
        "TEXT.build_vocab(train_data, max_size=MAX_VOCAB_SIZE)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWuYQthC4Ml8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "7f054b95-264a-4afa-85df-5275a8ccbacf"
      },
      "source": [
        "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
        "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in TEXT vocabulary: 25002\n",
            "Unique tokens in LABEL vocabulary: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW4eR-K44Rba",
        "colab_type": "text"
      },
      "source": [
        "### 出現頻度順で上位２０単語を見てみる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jan98ffr4PXP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a414642e-2aec-4552-94d2-b4b80b0b83b5"
      },
      "source": [
        "print(TEXT.vocab.freqs.most_common(20))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('the', 232028), (',', 219642), ('.', 189279), ('and', 125246), ('a', 124915), ('of', 115025), ('to', 106919), ('is', 87460), ('in', 70029), ('I', 62000), ('it', 61196), ('that', 56281), ('\"', 50475), (\"'s\", 49438), ('this', 48574), ('-', 42352), ('/><br', 40924), ('was', 39912), ('as', 34619), ('with', 34266)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKQojOuv4Z38",
        "colab_type": "text"
      },
      "source": [
        "### 単語ID順に最初の１０単語を見てみる\n",
        "* IDのうち、0と1は、未知語とパディング用の単語という特殊な単語に割り振られている。\n",
        " * パディングとは、長さが不揃いの複数の文書を同じミニバッチにまとめるとき、すべての文書の長さを無理やりそろえるため、文書末尾に特殊な単語（元々の語彙にない、人工的に用意した単語）を追加すること。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlhXRT3g4Xad",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "47766b8a-42ad-48ce-9cd8-6c295bdead1f"
      },
      "source": [
        "print(TEXT.vocab.itos[:10])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vJfHTdR4qd4",
        "colab_type": "text"
      },
      "source": [
        "### ラベルのほうのIDを確認する\n",
        "* こちらはnegとposに対応する２つのIDしかない。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI7Pz_6R4bYM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9c6c4a28-9ce5-447c-ae0b-41f07ae7c348"
      },
      "source": [
        "print(LABEL.vocab.stoi)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<function _default_unk_index at 0x7f35154642f0>, {'neg': 0, 'pos': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14_znTjp4w5s",
        "colab_type": "text"
      },
      "source": [
        "### ミニバッチを取り出すためのiteratorを作る\n",
        "* ミニバッチのサイズを指定する。\n",
        " * ミニバッチのサイズは、性能を出すためにチューニングすべきハイパーパラメータのひとつ。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUED86Jb4tUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 100\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, dev_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, dev_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAW9Ec5q6BQO",
        "colab_type": "text"
      },
      "source": [
        "### 試しにテストセットのiteratorを回してミニバッチをすべて取得して個数を数えてみる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kpn4tfWl42kY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fdc2f15f-14a6-4426-f53d-337c7174a698"
      },
      "source": [
        "i = 0\n",
        "for batch in test_iterator:\n",
        "  i += 1\n",
        "  continue\n",
        "print(f'We have {i} mini-batches in test set.')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 250 mini-batches in test set.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHytOsiSUdeS",
        "colab_type": "text"
      },
      "source": [
        "### ミニバッチの中身を見てみる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWW1np1P6OQg",
        "colab_type": "text"
      },
      "source": [
        "上記のループを抜けたあとには、変数batchにはテストセットの最後のミニバッチが代入されている。\n",
        "\n",
        "そこでこの最後のミニバッチのshapeを確認する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d78vJW616H7m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bc428617-c5d4-4a61-b69b-73a20c065878"
      },
      "source": [
        "batch.text.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 2640])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHMHkR73VuCD",
        "colab_type": "text"
      },
      "source": [
        "このミニバッチに含まれる文書のうち、最初のものの単語ID列と、先頭100個のIDを単語に戻したものを表示させてみる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tZLm0hQVjZE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "2737fdb6-671f-424f-c72b-825108416c36"
      },
      "source": [
        "print(batch.text[0])\n",
        "print(' '.join([TEXT.vocab.itos[i] for i in batch.text[0][:100]]))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([152,  15,   6,  ...,   7, 324,   4], device='cuda:0')\n",
            "There 's a sign on The Lost Highway that <unk> /><br <unk> SPOILERS <unk> /><br <unk> you already knew that , did n't <unk> /><br />Since there 's a great deal of people that apparently did not get the point of this movie , I 'd like to contribute my interpretation of why the plot makes perfect sense . As others have pointed out , one single viewing of this movie is not sufficient . If you have the DVD of <unk> , you can \" cheat \" by looking at David Lynch 's \" Top 10 <unk> to <unk>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtDXRKPMT9KW",
        "colab_type": "text"
      },
      "source": [
        "最後の文書の末尾は「1」で埋められていることが分かる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcdyIhK0TUac",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd511f0e-e874-4421-e036-b1ff9bc7204d"
      },
      "source": [
        "print(batch.text[BATCH_SIZE-1])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([  14,   25, 2884,  ...,    1,    1,    1], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDzk2ghCUD8N",
        "colab_type": "text"
      },
      "source": [
        "ミニバッチに含まれる文書の長さを調べると、文書が文書長の降順に並べられていることが分かる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PutP_EU4Tca-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "026583a6-7bda-46bc-bece-ca6a2d64cecd"
      },
      "source": [
        "(batch.text != 1).sum(1)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2640, 2537, 2534, 1915, 1635, 1364, 1324, 1290, 1287, 1284, 1279, 1274,\n",
              "        1264, 1255, 1253, 1241, 1241, 1232, 1229, 1226, 1225, 1221, 1213, 1212,\n",
              "        1211, 1210, 1206, 1205, 1198, 1196, 1195, 1193, 1193, 1193, 1191, 1190,\n",
              "        1190, 1187, 1185, 1183, 1182, 1181, 1181, 1181, 1181, 1179, 1179, 1177,\n",
              "        1176, 1175, 1175, 1175, 1173, 1173, 1173, 1172, 1172, 1172, 1171, 1170,\n",
              "        1169, 1169, 1168, 1167, 1166, 1166, 1164, 1164, 1163, 1163, 1162, 1161,\n",
              "        1159, 1158, 1158, 1157, 1157, 1155, 1155, 1153, 1152, 1152, 1151, 1150,\n",
              "        1149, 1148, 1147, 1146, 1145, 1144, 1144, 1144, 1144, 1144, 1144, 1143,\n",
              "        1143, 1142, 1142, 1142], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PDZlF0O6doP",
        "colab_type": "text"
      },
      "source": [
        "## 07-02 MLPによる文書分類の準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjpel2i46gbD",
        "colab_type": "text"
      },
      "source": [
        "### 定数の設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQPXVLC66NUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "NUM_CLASS = len(LABEL.vocab)\n",
        "EMBED_DIM = 100\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsuHjuNp6tvt",
        "colab_type": "text"
      },
      "source": [
        "### モデルを定義する前にPyTorchの単語埋め込みがどんなものかを見てみる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3J7TzxFVMsR",
        "colab_type": "text"
      },
      "source": [
        "以下のように、語彙サイズと埋め込み次元数を指定しつつ、torch.nn.Embeddingのインスタンスを作ればよい。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP7jJVYT6tBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed = nn.Embedding(INPUT_DIM, EMBED_DIM, padding_idx=PAD_IDX)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUl6lR8JVWTu",
        "colab_type": "text"
      },
      "source": [
        "パディング用の単語の埋め込みはゼロベクトルになる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3ZCr9Ll61m8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bee16876-5a09-4d93-8439-c52462eb7343"
      },
      "source": [
        "print(embed(torch.tensor([[0,2,1],[2,3,4]])))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.3374, -0.1778, -0.3035, -0.5880,  0.3486,  0.6603, -0.2196,\n",
            "          -0.3792,  0.7671, -1.1925,  0.6984, -1.4097,  0.1794,  1.8951,\n",
            "           0.4954,  0.2692, -0.0770, -1.0205, -0.1690,  0.9178,  1.5810,\n",
            "           1.3010,  1.2753, -0.2010,  0.4965, -1.5723,  0.9666, -1.1481,\n",
            "          -1.1589,  0.3255, -0.6315, -2.8400, -1.3250,  0.1784, -2.1338,\n",
            "           1.0524, -0.3885, -0.9343, -0.4991, -1.0867,  0.8805,  1.5542,\n",
            "           0.6266, -0.1755,  0.0983, -0.0935,  0.2662, -0.5850,  0.8768,\n",
            "           1.6221, -1.4779,  1.1331, -1.2203,  1.3139,  1.0533,  0.1388,\n",
            "           2.2473, -0.8036, -0.2808,  0.7697, -0.6596, -0.7979,  0.1838,\n",
            "           0.2293,  0.5146,  0.9938, -0.2587, -1.0826, -0.0444,  1.6236,\n",
            "          -2.3229,  1.0878,  0.6716,  0.6933, -0.9487, -0.0765, -0.1526,\n",
            "           0.1167,  0.4403, -1.4465,  0.2553, -0.5496,  1.0042,  0.8272,\n",
            "          -0.3948,  0.4892, -0.2168, -1.7472, -1.6025, -1.0764,  0.9031,\n",
            "          -0.7218, -0.5951, -0.7112,  0.6230, -1.3729, -2.2150, -1.3193,\n",
            "          -2.0915,  0.9629],\n",
            "         [ 1.1721, -0.4372, -0.4053,  0.7086,  0.9533, -0.0130, -0.1301,\n",
            "          -0.0877, -0.0673,  0.2467, -0.9392, -1.0448,  1.2783,  0.4190,\n",
            "          -0.5073, -0.6062, -1.0532,  1.8386, -0.1095, -0.3316,  0.9008,\n",
            "           0.4840, -1.3237,  0.7869,  1.3818, -0.0694, -0.7612,  0.2416,\n",
            "          -0.5878, -1.1506,  1.0164,  0.1234,  1.1311, -0.0858, -0.0597,\n",
            "           0.3553, -1.4355,  0.0727,  0.1053, -1.0311,  1.3113, -0.0360,\n",
            "           0.2118, -0.0086,  1.8576,  2.1321, -0.5056, -0.7988, -1.0944,\n",
            "          -1.0197, -0.5399,  1.2117, -0.8632,  1.3337,  0.0771, -0.0522,\n",
            "           0.2386,  0.1411, -1.3354, -2.9340,  0.1141, -1.2072, -0.3008,\n",
            "           0.1427, -1.3027, -0.4919, -2.1429,  0.9488, -0.5684, -0.0646,\n",
            "           0.6647, -2.7836,  1.1366,  0.9089,  0.9494,  0.0266, -0.9221,\n",
            "           0.7034, -0.3659, -0.1965, -0.9207,  0.3154, -0.0217,  0.3441,\n",
            "           0.2271, -0.4597, -0.6183,  0.2461, -0.4055, -0.8368,  1.2277,\n",
            "          -0.4297, -2.2121, -0.3780,  0.9838, -1.0895,  0.2017,  0.0221,\n",
            "          -1.7753, -0.7490],\n",
            "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000]],\n",
            "\n",
            "        [[ 1.1721, -0.4372, -0.4053,  0.7086,  0.9533, -0.0130, -0.1301,\n",
            "          -0.0877, -0.0673,  0.2467, -0.9392, -1.0448,  1.2783,  0.4190,\n",
            "          -0.5073, -0.6062, -1.0532,  1.8386, -0.1095, -0.3316,  0.9008,\n",
            "           0.4840, -1.3237,  0.7869,  1.3818, -0.0694, -0.7612,  0.2416,\n",
            "          -0.5878, -1.1506,  1.0164,  0.1234,  1.1311, -0.0858, -0.0597,\n",
            "           0.3553, -1.4355,  0.0727,  0.1053, -1.0311,  1.3113, -0.0360,\n",
            "           0.2118, -0.0086,  1.8576,  2.1321, -0.5056, -0.7988, -1.0944,\n",
            "          -1.0197, -0.5399,  1.2117, -0.8632,  1.3337,  0.0771, -0.0522,\n",
            "           0.2386,  0.1411, -1.3354, -2.9340,  0.1141, -1.2072, -0.3008,\n",
            "           0.1427, -1.3027, -0.4919, -2.1429,  0.9488, -0.5684, -0.0646,\n",
            "           0.6647, -2.7836,  1.1366,  0.9089,  0.9494,  0.0266, -0.9221,\n",
            "           0.7034, -0.3659, -0.1965, -0.9207,  0.3154, -0.0217,  0.3441,\n",
            "           0.2271, -0.4597, -0.6183,  0.2461, -0.4055, -0.8368,  1.2277,\n",
            "          -0.4297, -2.2121, -0.3780,  0.9838, -1.0895,  0.2017,  0.0221,\n",
            "          -1.7753, -0.7490],\n",
            "         [ 0.2781, -0.9621, -0.4223, -1.1036,  0.2473,  1.4549, -0.2835,\n",
            "          -0.3767, -0.0306, -0.0894, -0.1965, -0.9713,  0.9005, -0.2523,\n",
            "           1.0669, -0.2985,  0.8558,  1.6098, -1.1893,  1.1677,  0.3277,\n",
            "          -0.8331, -1.6179,  0.2265, -0.4382,  0.3265, -1.5786, -1.3995,\n",
            "           0.5446, -0.0830, -1.1753,  1.7825,  1.7524, -0.2135,  0.4095,\n",
            "           0.0465,  0.6367, -0.1943, -0.8614,  0.5338,  0.9376, -0.9225,\n",
            "           0.7047, -0.2722,  0.0144, -0.6411,  2.3902, -1.4256, -0.4619,\n",
            "          -1.5539, -0.3338,  0.2405,  2.1065,  0.5509, -0.2936, -1.8027,\n",
            "          -0.6933,  1.7409,  0.2698,  0.9595, -1.0253, -0.5505,  1.0264,\n",
            "          -0.5670, -0.2658, -1.1116, -1.3696, -0.6534, -1.6125, -0.2284,\n",
            "           1.8388, -0.9473,  0.1419,  0.3696, -0.0174, -0.9575, -0.8169,\n",
            "          -0.2866,  0.4343, -0.1340, -2.1467, -1.7984, -0.6822, -0.5191,\n",
            "           0.0093, -1.8110, -0.2443,  0.1327,  1.0875, -0.1029,  0.8604,\n",
            "           0.2078,  0.2027,  0.5021, -0.4063,  0.6664,  0.4765, -1.4498,\n",
            "           1.5446,  1.0394],\n",
            "         [ 2.1681,  0.4884,  0.3359, -1.2282, -0.1200,  0.4884,  1.9431,\n",
            "           0.2169, -0.4743, -0.3679, -0.2918, -1.6531,  0.7692, -1.1323,\n",
            "           2.9590,  0.8171,  0.7668,  1.3258,  0.2103,  1.7876, -1.2128,\n",
            "           0.2045,  1.1051, -0.5454,  0.1073,  0.8727, -1.2800, -0.4619,\n",
            "           1.4342, -1.2103,  1.3834,  0.0324,  0.5421,  0.8796,  0.2713,\n",
            "           1.6067, -1.0004,  0.7392, -0.4931,  0.4073, -1.0394, -0.3226,\n",
            "           0.7226,  0.2674, -0.4673,  0.6916, -1.8752,  0.3008, -0.1468,\n",
            "           1.3672,  0.7074,  0.3276,  1.0658,  1.4130, -1.2445,  0.2227,\n",
            "           0.4593, -0.3845,  0.6554, -0.1045, -1.1134,  0.5110,  0.3566,\n",
            "           1.8591, -0.9300,  1.1186,  1.7495,  2.3058,  0.3734,  0.3314,\n",
            "          -0.1871,  0.1770,  2.9641,  0.2307,  0.3228,  0.2610,  0.3219,\n",
            "           1.7745,  0.3155, -0.9364,  0.5687, -0.0959,  0.0046, -1.4321,\n",
            "          -0.1535, -0.1925, -0.3115, -0.1812, -0.8745, -0.0270,  0.5424,\n",
            "           1.3656, -0.0284, -0.7411, -0.0169,  1.7024,  0.4206,  0.9317,\n",
            "           0.9884, -0.3948]]], grad_fn=<EmbeddingBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGyngitc78hv",
        "colab_type": "text"
      },
      "source": [
        "### モデルの定義\n",
        "* 基本的にMLPだが、入り口に単語埋め込み層が挿入されている。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9asdLYng7DOu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EmbedTextSentiment(nn.Module):\n",
        "  def __init__(self, embed_dim, num_class, vocab_size, padding_idx):\n",
        "    super(EmbedTextSentiment, self).__init__()\n",
        "    self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
        "    self.fc1 = nn.Linear(embed_dim, 500)\n",
        "    self.fc2 = nn.Linear(500, 100)\n",
        "    self.fc3 = nn.Linear(100, num_class)\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "    initrange = 0.5\n",
        "    self.fc1.weight.data.uniform_(-initrange, initrange)\n",
        "    self.fc1.bias.data.zero_()\n",
        "    self.fc2.weight.data.uniform_(-initrange, initrange)\n",
        "    self.fc2.bias.data.zero_()\n",
        "    self.fc3.weight.data.uniform_(-initrange, initrange)\n",
        "    self.fc3.bias.data.zero_()\n",
        "\n",
        "  def forward(self, text):\n",
        "    x = self.embed(text)\n",
        "    x = x.mean(1) \n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foU72cB48IO9",
        "colab_type": "text"
      },
      "source": [
        "### モデルを作る\n",
        "* モデル（のインスタンス）をGPUに移動させている点に注意。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0BHCGAZ8F18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = EmbedTextSentiment(EMBED_DIM, NUM_CLASS, INPUT_DIM, padding_idx=PAD_IDX).to(device)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wylQOq8N8cqI",
        "colab_type": "text"
      },
      "source": [
        "### 損失関数とoptimizerとschedulerを作る\n",
        "* 損失関数をGPUに移動させている点に注意。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gw34INS78cIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilWLfu8Z8MzW",
        "colab_type": "text"
      },
      "source": [
        "### 訓練用の関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR2R4Lqh8J7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(data_iterator, model, optimizer, scheduler, criterion):\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  train_loss = 0\n",
        "  train_acc = 0\n",
        "  for batch in data_iterator:\n",
        "    optimizer.zero_grad()\n",
        "    text, cls = batch.text, batch.label\n",
        "    output = model(text)\n",
        "    loss = criterion(output, cls)\n",
        "    train_loss += loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_acc += (output.argmax(1) == cls).sum().item()\n",
        "\n",
        "  scheduler.step()\n",
        "\n",
        "  data_len = len(data_iterator.dataset)\n",
        "  return train_loss / data_len, train_acc / data_len"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftuX8e1W8iRh",
        "colab_type": "text"
      },
      "source": [
        "### 評価用の関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGUnsJlq8Ue3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(data_iterator, model, criterion):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  loss = 0\n",
        "  acc = 0\n",
        "  for batch in data_iterator:\n",
        "    text, cls = batch.text, batch.label\n",
        "    with torch.no_grad():\n",
        "      output = model(text)\n",
        "      loss = criterion(output, cls)\n",
        "      loss += loss.item()\n",
        "      acc += (output.argmax(1) == cls).sum().item()\n",
        "\n",
        "  data_len = len(data_iterator.dataset)\n",
        "  return loss / data_len, acc / data_len"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8o_jDAg8osP",
        "colab_type": "text"
      },
      "source": [
        "## 07-03 分類器の訓練と開発セットでの評価"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJJFv4k-8mH1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 974
        },
        "outputId": "5b35d517-6b58-49fe-df0d-54722ec89f6c"
      },
      "source": [
        "N_EPOCHS = 20\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "  start_time = time.time()\n",
        "  train_loss, train_acc = train(train_iterator, model, optimizer, scheduler, criterion)\n",
        "  dev_loss, dev_acc = test(dev_iterator, model, criterion)\n",
        "\n",
        "  secs = int(time.time() - start_time)\n",
        "  mins = secs / 60\n",
        "  secs = secs % 60\n",
        "\n",
        "  print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
        "  print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
        "  print(f'\\tLoss: {dev_loss:.4f}(dev)\\t|\\tAcc: {dev_acc * 100:.1f}%(dev)')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0043(train)\t|\tAcc: 79.9%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.5%(dev)\n",
            "Epoch: 2  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0016(train)\t|\tAcc: 93.9%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.4%(dev)\n",
            "Epoch: 3  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0008(train)\t|\tAcc: 97.4%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 87.0%(dev)\n",
            "Epoch: 4  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0004(train)\t|\tAcc: 98.6%(train)\n",
            "\tLoss: 0.0003(dev)\t|\tAcc: 86.7%(dev)\n",
            "Epoch: 5  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0002(train)\t|\tAcc: 99.4%(train)\n",
            "\tLoss: 0.0003(dev)\t|\tAcc: 86.8%(dev)\n",
            "Epoch: 6  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0001(train)\t|\tAcc: 99.7%(train)\n",
            "\tLoss: 0.0004(dev)\t|\tAcc: 87.1%(dev)\n",
            "Epoch: 7  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0001(train)\t|\tAcc: 99.8%(train)\n",
            "\tLoss: 0.0004(dev)\t|\tAcc: 86.9%(dev)\n",
            "Epoch: 8  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 99.9%(train)\n",
            "\tLoss: 0.0005(dev)\t|\tAcc: 87.0%(dev)\n",
            "Epoch: 9  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0001(train)\t|\tAcc: 99.8%(train)\n",
            "\tLoss: 0.0005(dev)\t|\tAcc: 87.3%(dev)\n",
            "Epoch: 10  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 99.9%(train)\n",
            "\tLoss: 0.0005(dev)\t|\tAcc: 87.4%(dev)\n",
            "Epoch: 11  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0005(dev)\t|\tAcc: 86.9%(dev)\n",
            "Epoch: 12  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0006(dev)\t|\tAcc: 87.4%(dev)\n",
            "Epoch: 13  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0006(dev)\t|\tAcc: 87.4%(dev)\n",
            "Epoch: 14  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0006(dev)\t|\tAcc: 87.3%(dev)\n",
            "Epoch: 15  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0006(dev)\t|\tAcc: 87.4%(dev)\n",
            "Epoch: 16  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0006(dev)\t|\tAcc: 87.3%(dev)\n",
            "Epoch: 17  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0006(dev)\t|\tAcc: 87.3%(dev)\n",
            "Epoch: 18  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0007(dev)\t|\tAcc: 87.3%(dev)\n",
            "Epoch: 20  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0007(dev)\t|\tAcc: 87.3%(dev)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPux8PReWTXG",
        "colab_type": "text"
      },
      "source": [
        "## 07-04 再検討\n",
        "* 訓練データ上での分類精度が100%になってしまっている。\n",
        " * 明らかにオーバーフィッティングを起こしてしまっている。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23jMgtmoWkty",
        "colab_type": "text"
      },
      "source": [
        "### ドロップアウトを使う\n",
        "* モデルのインスタンスを作るときにdropoutの確率を引数pで指定できるようにする。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khps3ZuBWntq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EmbedTextSentiment(nn.Module):\n",
        "  def __init__(self, embed_dim, num_class, vocab_size, padding_idx, p=0.0):\n",
        "    super(EmbedTextSentiment, self).__init__()\n",
        "    self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
        "    self.dropout = nn.Dropout(p=p)\n",
        "    self.fc1 = nn.Linear(embed_dim, 500)\n",
        "    self.fc2 = nn.Linear(500, 100)\n",
        "    self.fc3 = nn.Linear(100, num_class)\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "    initrange = 0.5\n",
        "    self.fc1.weight.data.uniform_(-initrange, initrange)\n",
        "    self.fc1.bias.data.zero_()\n",
        "    self.fc2.weight.data.uniform_(-initrange, initrange)\n",
        "    self.fc2.bias.data.zero_()\n",
        "    self.fc3.weight.data.uniform_(-initrange, initrange)\n",
        "    self.fc3.bias.data.zero_()\n",
        "\n",
        "  def forward(self, text):\n",
        "    x = self.dropout(self.embed(text))\n",
        "    x = x.mean(1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVXbkt6qXxNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = EmbedTextSentiment(EMBED_DIM, NUM_CLASS, INPUT_DIM, padding_idx=PAD_IDX, p=0.5).to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXkBDXc6X1mp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "098e9b34-3803-4cd9-b8cc-d07d9ff885bb"
      },
      "source": [
        "N_EPOCHS = 20\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "  start_time = time.time()\n",
        "  train_loss, train_acc = train(train_iterator, model, optimizer, scheduler, criterion)\n",
        "  dev_loss, dev_acc = test(dev_iterator, model, criterion)\n",
        "\n",
        "  secs = int(time.time() - start_time)\n",
        "  mins = secs / 60\n",
        "  secs = secs % 60\n",
        "\n",
        "  print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
        "  print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
        "  print(f'\\tLoss: {dev_loss:.4f}(dev)\\t|\\tAcc: {dev_acc * 100:.1f}%(dev)')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0050(train)\t|\tAcc: 75.3%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.0%(dev)\n",
            "Epoch: 2  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0024(train)\t|\tAcc: 90.2%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.9%(dev)\n",
            "Epoch: 3  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0016(train)\t|\tAcc: 93.8%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.4%(dev)\n",
            "Epoch: 4  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0012(train)\t|\tAcc: 95.4%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.9%(dev)\n",
            "Epoch: 5  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0009(train)\t|\tAcc: 96.5%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.4%(dev)\n",
            "Epoch: 6  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0007(train)\t|\tAcc: 97.4%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 88.6%(dev)\n",
            "Epoch: 7  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0006(train)\t|\tAcc: 97.5%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.6%(dev)\n",
            "Epoch: 8  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0005(train)\t|\tAcc: 98.2%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 88.3%(dev)\n",
            "Epoch: 9  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0004(train)\t|\tAcc: 98.4%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 88.4%(dev)\n",
            "Epoch: 10  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0003(train)\t|\tAcc: 98.7%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 88.3%(dev)\n",
            "Epoch: 11  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0003(train)\t|\tAcc: 98.8%(train)\n",
            "\tLoss: 0.0003(dev)\t|\tAcc: 88.0%(dev)\n",
            "Epoch: 12  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0003(train)\t|\tAcc: 98.9%(train)\n",
            "\tLoss: 0.0003(dev)\t|\tAcc: 88.3%(dev)\n",
            "Epoch: 13  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0002(train)\t|\tAcc: 99.2%(train)\n",
            "\tLoss: 0.0003(dev)\t|\tAcc: 88.2%(dev)\n",
            "Epoch: 14  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0002(train)\t|\tAcc: 99.3%(train)\n",
            "\tLoss: 0.0003(dev)\t|\tAcc: 88.3%(dev)\n",
            "Epoch: 15  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0002(train)\t|\tAcc: 99.2%(train)\n",
            "\tLoss: 0.0003(dev)\t|\tAcc: 87.9%(dev)\n",
            "Epoch: 16  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0002(train)\t|\tAcc: 99.4%(train)\n",
            "\tLoss: 0.0003(dev)\t|\tAcc: 87.7%(dev)\n",
            "Epoch: 17  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0002(train)\t|\tAcc: 99.3%(train)\n",
            "\tLoss: 0.0004(dev)\t|\tAcc: 88.0%(dev)\n",
            "Epoch: 18  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0002(train)\t|\tAcc: 99.5%(train)\n",
            "\tLoss: 0.0004(dev)\t|\tAcc: 87.7%(dev)\n",
            "Epoch: 19  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0002(train)\t|\tAcc: 99.5%(train)\n",
            "\tLoss: 0.0004(dev)\t|\tAcc: 87.9%(dev)\n",
            "Epoch: 20  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0001(train)\t|\tAcc: 99.5%(train)\n",
            "\tLoss: 0.0004(dev)\t|\tAcc: 87.7%(dev)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu3Y-wjwb0po",
        "colab_type": "text"
      },
      "source": [
        "### L２正則化を使う\n",
        "* optimizerのweight_decayパラメータを0より大きな値にする。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmxEuSFJazCJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = EmbedTextSentiment(EMBED_DIM, NUM_CLASS, INPUT_DIM, padding_idx=PAD_IDX, p=0.5).to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0Zr2S7ga3J4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d6029182-2ce3-4d98-c1e2-613a4ae2f1dd"
      },
      "source": [
        "N_EPOCHS = 20\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "  start_time = time.time()\n",
        "  train_loss, train_acc = train(train_iterator, model, optimizer, scheduler, criterion)\n",
        "  dev_loss, dev_acc = test(dev_iterator, model, criterion)\n",
        "\n",
        "  secs = int(time.time() - start_time)\n",
        "  mins = secs / 60\n",
        "  secs = secs % 60\n",
        "\n",
        "  print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
        "  print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
        "  print(f'\\tLoss: {dev_loss:.4f}(dev)\\t|\\tAcc: {dev_acc * 100:.1f}%(dev)')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0066(train)\t|\tAcc: 62.8%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 75.9%(dev)\n",
            "Epoch: 2  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0051(train)\t|\tAcc: 75.7%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 81.7%(dev)\n",
            "Epoch: 3  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0045(train)\t|\tAcc: 79.3%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 84.3%(dev)\n",
            "Epoch: 4  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0041(train)\t|\tAcc: 82.7%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 81.4%(dev)\n",
            "Epoch: 5  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0039(train)\t|\tAcc: 83.6%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 85.6%(dev)\n",
            "Epoch: 6  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0036(train)\t|\tAcc: 85.1%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 85.4%(dev)\n",
            "Epoch: 7  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0036(train)\t|\tAcc: 85.3%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 86.9%(dev)\n",
            "Epoch: 8  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0034(train)\t|\tAcc: 86.3%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 85.9%(dev)\n",
            "Epoch: 9  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0033(train)\t|\tAcc: 87.2%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.2%(dev)\n",
            "Epoch: 10  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0031(train)\t|\tAcc: 87.6%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.1%(dev)\n",
            "Epoch: 11  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0031(train)\t|\tAcc: 88.1%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.8%(dev)\n",
            "Epoch: 12  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0030(train)\t|\tAcc: 88.7%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.7%(dev)\n",
            "Epoch: 13  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0028(train)\t|\tAcc: 89.3%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.6%(dev)\n",
            "Epoch: 14  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0028(train)\t|\tAcc: 89.2%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.5%(dev)\n",
            "Epoch: 15  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0027(train)\t|\tAcc: 89.7%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.8%(dev)\n",
            "Epoch: 16  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0026(train)\t|\tAcc: 90.6%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.9%(dev)\n",
            "Epoch: 17  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0025(train)\t|\tAcc: 90.6%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.1%(dev)\n",
            "Epoch: 18  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0024(train)\t|\tAcc: 91.1%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.2%(dev)\n",
            "Epoch: 19  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0023(train)\t|\tAcc: 91.6%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.1%(dev)\n",
            "Epoch: 20  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0023(train)\t|\tAcc: 91.8%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.2%(dev)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIHA64UTdmBj",
        "colab_type": "text"
      },
      "source": [
        "### early stopping\n",
        "* dev setでのaccuracyが4回連続で最高値を下回ったら訓練を終えることにする。\n",
        "* early stoppingの実現については、PyTorch Lightningを使う手もある。\n",
        " * https://pytorch-lightning.readthedocs.io/en/latest/early_stopping.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0zclQnVdlVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = EmbedTextSentiment(EMBED_DIM, NUM_CLASS, INPUT_DIM, padding_idx=PAD_IDX, p=0.5).to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3E_I5sRc3FF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "17083af0-fe95-4781-e0e6-7a3ca287cf1d"
      },
      "source": [
        "patience = 4\n",
        "early_stop_count = 0\n",
        "best_dev_acc = 0.0\n",
        "dev_acc_threshold = 0.87\n",
        "\n",
        "N_EPOCHS = 50 # エポック数を増やしておく\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "  start_time = time.time()\n",
        "  train_loss, train_acc = train(train_iterator, model, optimizer, scheduler, criterion)\n",
        "  dev_loss, dev_acc = test(dev_iterator, model, criterion)\n",
        "\n",
        "  secs = int(time.time() - start_time)\n",
        "  mins = secs / 60\n",
        "  secs = secs % 60\n",
        "\n",
        "  print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
        "  print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
        "  print(f'\\tLoss: {dev_loss:.4f}(dev)\\t|\\tAcc: {dev_acc * 100:.1f}%(dev)')\n",
        "\n",
        "  # early stopping\n",
        "  if best_dev_acc <= dev_acc:\n",
        "    best_dev_acc = dev_acc\n",
        "    early_stop_count = 0\n",
        "  else:\n",
        "    early_stop_count += 1\n",
        "    if early_stop_count == patience:\n",
        "      break"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0066(train)\t|\tAcc: 63.9%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 75.1%(dev)\n",
            "Epoch: 2  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0051(train)\t|\tAcc: 75.3%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 80.8%(dev)\n",
            "Epoch: 3  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0046(train)\t|\tAcc: 78.7%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 79.6%(dev)\n",
            "Epoch: 4  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0041(train)\t|\tAcc: 81.7%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 82.8%(dev)\n",
            "Epoch: 5  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0039(train)\t|\tAcc: 83.4%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 85.5%(dev)\n",
            "Epoch: 6  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0037(train)\t|\tAcc: 84.3%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 85.7%(dev)\n",
            "Epoch: 7  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0035(train)\t|\tAcc: 86.0%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 85.1%(dev)\n",
            "Epoch: 8  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0034(train)\t|\tAcc: 85.9%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 86.5%(dev)\n",
            "Epoch: 9  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0033(train)\t|\tAcc: 86.5%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 86.4%(dev)\n",
            "Epoch: 10  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0032(train)\t|\tAcc: 87.3%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 85.9%(dev)\n",
            "Epoch: 11  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0031(train)\t|\tAcc: 87.9%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.6%(dev)\n",
            "Epoch: 12  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0029(train)\t|\tAcc: 88.9%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.3%(dev)\n",
            "Epoch: 13  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0028(train)\t|\tAcc: 89.2%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.1%(dev)\n",
            "Epoch: 14  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0028(train)\t|\tAcc: 89.3%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 86.7%(dev)\n",
            "Epoch: 15  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0027(train)\t|\tAcc: 90.0%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.7%(dev)\n",
            "Epoch: 16  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0026(train)\t|\tAcc: 90.6%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.6%(dev)\n",
            "Epoch: 17  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0025(train)\t|\tAcc: 90.9%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.0%(dev)\n",
            "Epoch: 18  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0024(train)\t|\tAcc: 91.2%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.2%(dev)\n",
            "Epoch: 19  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0023(train)\t|\tAcc: 91.8%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.3%(dev)\n",
            "Epoch: 20  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0023(train)\t|\tAcc: 92.0%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.4%(dev)\n",
            "Epoch: 21  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0022(train)\t|\tAcc: 92.3%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.4%(dev)\n",
            "Epoch: 22  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0022(train)\t|\tAcc: 92.4%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.5%(dev)\n",
            "Epoch: 23  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0021(train)\t|\tAcc: 92.7%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.5%(dev)\n",
            "Epoch: 24  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0021(train)\t|\tAcc: 93.0%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.4%(dev)\n",
            "Epoch: 25  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0020(train)\t|\tAcc: 93.4%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.6%(dev)\n",
            "Epoch: 26  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0020(train)\t|\tAcc: 93.6%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.6%(dev)\n",
            "Epoch: 27  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0019(train)\t|\tAcc: 93.8%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.8%(dev)\n",
            "Epoch: 28  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0019(train)\t|\tAcc: 93.7%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.7%(dev)\n",
            "Epoch: 29  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0019(train)\t|\tAcc: 94.2%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.6%(dev)\n",
            "Epoch: 30  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0019(train)\t|\tAcc: 94.1%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.6%(dev)\n",
            "Epoch: 31  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0018(train)\t|\tAcc: 94.4%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.5%(dev)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adAOHJ-re13B",
        "colab_type": "text"
      },
      "source": [
        "ここで戻ってweight_decayやdropoutの確率などをチューニングし直す。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMqQ89Cpe7jJ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRvkncN09MKk",
        "colab_type": "text"
      },
      "source": [
        "## 07-05 テストセット上で評価\n",
        "* 見つけ出したベストな設定を使って、テストセット上での最終的な評価をおこなう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39XBHTCGhlUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ベストな設定を使っての学習のやり直しのコードをここに書く"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_gHj4x38y8h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "67e228f5-5c38-4a41-b272-7d6ac2a7ff83"
      },
      "source": [
        "print('Checking the results of test dataset...')\n",
        "test_loss, test_acc = test(test_iterator, model, criterion)\n",
        "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking the results of test dataset...\n",
            "\tLoss: 0.0000(test)\t|\tAcc: 88.3%(test)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1M_VQ1xhcWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 37,
      "outputs": []
    }
  ]
}