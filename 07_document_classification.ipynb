{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07_document_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNM2NYsPvN//zBqwMvp5zR8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course-nlp2020/blob/master/07_document_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_0ZZ8bo1mgH",
        "colab_type": "text"
      },
      "source": [
        "# 07 文書分類\n",
        "* 今回は、fastTextのような学習済みの単語埋め込みは使わない。\n",
        "* 単語埋め込み自体の学習も、ネットワークの重みの学習と同時におこなう。\n",
        "* IMDbデータの準備も、PyTorchのtorchtextモジュールを使っておこなう。\n",
        "* ネットワークへの入力は、単語埋め込みを、単語の出現順どおりに並べた列にする。\n",
        "* そして、前向き計算のなかではじめて、単語埋め込みの平均をとることにする。\n",
        "* 参考資料\n",
        " * https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
        " * https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb\n",
        " * https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_puYg6Zi8x3",
        "colab_type": "text"
      },
      "source": [
        "## 07-00 下準備\n",
        "* Google ColabのランタイムのタイプをGPUに変更しておこう。\n",
        " * 上のメニューの「ランタイム」→「ランタイムのタイプを変更」→「ハードウェア　アクセラレータ」から「GPU」を選択"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLEeO0fw23Xp",
        "colab_type": "text"
      },
      "source": [
        "## 07-01 torchtextを使ってIMDbデータを読み込む\n",
        "* https://torchtext.readthedocs.io/en/latest/datasets.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go7epLZe3JmF",
        "colab_type": "text"
      },
      "source": [
        "### 実験の再現性確保のための設定など\n",
        "* torch.backends.cudnn.deterministicをTrueにするのは、こうしないと、GPU上での計算が毎回同じ値を与えないため。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nSqNzof1lTJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "\n",
        "SEED = 123\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y1_GyXg22f6",
        "colab_type": "text"
      },
      "source": [
        "### フィールドのインスタンスを作る\n",
        "* TEXTフィールドと、LABELフィールドという２種類のFieldオブジェクトのインスタンスを作る。\n",
        "* TEXTフィールドは、テキストの前処理の仕方を決めておくのに使う。\n",
        " * batch_firstをTrueに設定するのが今回のポイント。\n",
        " * tokenizerは、デフォルトでは単にstring型のsplitメソッドを適用するだけになる。これは高速だが、tokenizationとしては雑。\n",
        "* LABELフィールドは、ラベルの前処理に使う。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjq8oooE2uQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = data.Field(tokenize=\"spacy\", batch_first=True)\n",
        "LABEL = data.LabelField()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtEq23GS3Vxl",
        "colab_type": "text"
      },
      "source": [
        "### IMDbデータセットを前処理しつつ読み込む\n",
        "* TEXTフィールドでspaCyのtokenizationを使うように設定したので、少し時間がかかる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzgVXf3G3YPI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "16d3b280-5698-46e0-f1b3-5f6b39911d88"
      },
      "source": [
        "train_dev_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz:   0%|          | 98.3k/84.1M [00:00<01:28, 946kB/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:01<00:00, 58.4MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0sltPjT3j36",
        "colab_type": "text"
      },
      "source": [
        "### 最初の文書を見てみる\n",
        "* `vars`関数は、モジュール、クラス、インスタンス、あるいはそれ以外の`__dict__`属性を持つオブジェクトの、`__dict__`属性を辞書として返す組み込み関数。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "692vrq6B3gZe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2dcadf30-7909-4aa2-c509-97b74307a7f5"
      },
      "source": [
        "print(vars(train_dev_data.examples[0]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': ['I', 'really', 'liked', 'this', 'movie', ',', 'and', 'went', 'back', 'to', 'see', 'it', 'two', 'times', 'more', 'within', 'a', 'week.<br', '/><br', '/>Ms', '.', 'Detmers', 'nailed', 'the', 'performance', '-', 'she', 'was', 'like', 'a', 'hungry', 'cat', 'on', 'the', 'prowl', ',', 'toying', 'with', 'her', 'prey', '.', 'She', 'lashes', 'out', 'in', 'rage', 'and', 'lust', ',', 'taking', 'a', '\"', 'too', 'young', '\"', 'lover', ',', 'and', 'crashing', 'hundreds', 'of', 'her', 'terrorist', 'fiancé', \"'s\", 'mother', \"'s\", 'pieces', 'of', 'fine', 'china', 'to', 'the', 'floor', '.', '<', 'br', '/><br', '/>The', 'film', 'was', 'full', 'of', 'beautiful', 'touches', '.', 'The', 'Maserati', ',', 'the', 'wonderful', 'wardrobe', ',', 'the', 'flower', 'boxes', 'along', 'the', 'rooftops', '.', 'I', 'particularly', 'enjoyed', 'the', 'ancient', 'Greek', 'class', 'and', 'the', 'recitation', 'of', \"'\", \"Antigone'.<br\", '/><br', '/>It', 'had', 'a', 'feeling', 'of', \"'\", 'Story', 'of', 'O', \"'\", '-', 'that', 'is', ',', 'where', 'people', 'of', 'means', 'indulge', 'in', 'unrestrained', 'sexual', 'adventure', '.', 'As', 'she', 'walks', 'around', 'the', 'fantastic', 'apartment', 'in', 'the', 'buff', ',', 'she', 'is', 'at', 'ease', '-', 'and', 'why', 'not', ',', 'what', 'is', 'to', 'restrain', 'a', '\"', 'Devil', 'in', 'the', 'Flesh\"?<br', '/><br', '/>The', 'whole', 'movie', 'is', 'a', 'real', 'treat', '!'], 'label': 'pos'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrXwYMVH3orf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6430f6ef-709f-4142-ac75-6d91faf71dd3"
      },
      "source": [
        "print(train_dev_data.examples[0].text)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'really', 'liked', 'this', 'movie', ',', 'and', 'went', 'back', 'to', 'see', 'it', 'two', 'times', 'more', 'within', 'a', 'week.<br', '/><br', '/>Ms', '.', 'Detmers', 'nailed', 'the', 'performance', '-', 'she', 'was', 'like', 'a', 'hungry', 'cat', 'on', 'the', 'prowl', ',', 'toying', 'with', 'her', 'prey', '.', 'She', 'lashes', 'out', 'in', 'rage', 'and', 'lust', ',', 'taking', 'a', '\"', 'too', 'young', '\"', 'lover', ',', 'and', 'crashing', 'hundreds', 'of', 'her', 'terrorist', 'fiancé', \"'s\", 'mother', \"'s\", 'pieces', 'of', 'fine', 'china', 'to', 'the', 'floor', '.', '<', 'br', '/><br', '/>The', 'film', 'was', 'full', 'of', 'beautiful', 'touches', '.', 'The', 'Maserati', ',', 'the', 'wonderful', 'wardrobe', ',', 'the', 'flower', 'boxes', 'along', 'the', 'rooftops', '.', 'I', 'particularly', 'enjoyed', 'the', 'ancient', 'Greek', 'class', 'and', 'the', 'recitation', 'of', \"'\", \"Antigone'.<br\", '/><br', '/>It', 'had', 'a', 'feeling', 'of', \"'\", 'Story', 'of', 'O', \"'\", '-', 'that', 'is', ',', 'where', 'people', 'of', 'means', 'indulge', 'in', 'unrestrained', 'sexual', 'adventure', '.', 'As', 'she', 'walks', 'around', 'the', 'fantastic', 'apartment', 'in', 'the', 'buff', ',', 'she', 'is', 'at', 'ease', '-', 'and', 'why', 'not', ',', 'what', 'is', 'to', 'restrain', 'a', '\"', 'Devil', 'in', 'the', 'Flesh\"?<br', '/><br', '/>The', 'whole', 'movie', 'is', 'a', 'real', 'treat', '!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRMuAOum3rB5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "758ec4bc-a1a9-41c3-f325-e2ec56963346"
      },
      "source": [
        "print(train_dev_data.examples[0].label)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgZgQbyD3u9D",
        "colab_type": "text"
      },
      "source": [
        "### テストセット以外の部分を訓練データと検証データに分ける"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2FtnEKZ32hM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, dev_data = train_dev_data.split(split_ratio=0.8, random_state = random.seed(SEED))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fzsi9ZC36eR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "542f8b34-c12a-42b3-aab2-b92823c734ef"
      },
      "source": [
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of development examples: {len(dev_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 20000\n",
            "Number of development examples: 5000\n",
            "Number of testing examples: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oXz2lvB37Vm",
        "colab_type": "text"
      },
      "source": [
        "### データセットのラベルを作る\n",
        "* TEXTラベルのほうでは、最大語彙サイズを指定する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBQeD7yC37x4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_VOCAB_SIZE = 25000\n",
        "\n",
        "TEXT.build_vocab(train_data, max_size=MAX_VOCAB_SIZE)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWuYQthC4Ml8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "d28d0067-1a51-4311-cadb-bbaee2249fbd"
      },
      "source": [
        "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
        "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in TEXT vocabulary: 25002\n",
            "Unique tokens in LABEL vocabulary: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW4eR-K44Rba",
        "colab_type": "text"
      },
      "source": [
        "### 出現頻度順で上位２０単語を見てみる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jan98ffr4PXP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "bf8500af-c313-4905-8db8-c17d49f02efa"
      },
      "source": [
        "print(TEXT.vocab.freqs.most_common(20))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('the', 232028), (',', 219642), ('.', 189279), ('and', 125246), ('a', 124915), ('of', 115025), ('to', 106919), ('is', 87460), ('in', 70029), ('I', 62000), ('it', 61196), ('that', 56281), ('\"', 50475), (\"'s\", 49438), ('this', 48574), ('-', 42352), ('/><br', 40924), ('was', 39912), ('as', 34619), ('with', 34266)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKQojOuv4Z38",
        "colab_type": "text"
      },
      "source": [
        "### 単語ID順に最初の１０単語を見てみる\n",
        "* IDのうち、0と1は、未知語とパディング用の単語という特殊な単語に割り振られている。\n",
        " * パディングとは、長さが不揃いの複数の文書を同じミニバッチにまとめるとき、すべての文書の長さを無理やりそろえるため、文書末尾に特殊な単語（元々の語彙にない、人工的に用意した単語）を追加すること。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlhXRT3g4Xad",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ab851948-cc36-4ca8-ea3e-f46a20e06b53"
      },
      "source": [
        "print(TEXT.vocab.itos[:10])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vJfHTdR4qd4",
        "colab_type": "text"
      },
      "source": [
        "### ラベルのほうのIDを確認する\n",
        "* こちらはnegとposに対応する２つのIDしかない。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI7Pz_6R4bYM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c3f2bd70-d4b6-4fb0-94f4-530d86279fd7"
      },
      "source": [
        "print(LABEL.vocab.stoi)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<function _default_unk_index at 0x7f23b9dca2f0>, {'neg': 0, 'pos': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14_znTjp4w5s",
        "colab_type": "text"
      },
      "source": [
        "### ミニバッチを取り出すためのiteratorを作る\n",
        "* ミニバッチのサイズを指定する。\n",
        " * ミニバッチのサイズは、性能を出すためにチューニングすべきハイパーパラメータのひとつ。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUED86Jb4tUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 100\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, dev_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, dev_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAW9Ec5q6BQO",
        "colab_type": "text"
      },
      "source": [
        "### 試しにテストセットのiteratorを回してミニバッチをすべて取得して個数を数えてみる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kpn4tfWl42kY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "87330429-0f1e-4544-9ab3-8e18bdc3e547"
      },
      "source": [
        "i = 0\n",
        "for batch in test_iterator:\n",
        "  i += 1\n",
        "  continue\n",
        "print(f'We have {i} mini-batches in test set.')\n",
        "print(batch.text[0])\n",
        "print(' '.join([TEXT.vocab.itos[i] for i in batch.text[0]]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 250 mini-batches in test set.\n",
            "tensor([152,  15,   6,  ...,   7, 324,   4], device='cuda:0')\n",
            "There 's a sign on The Lost Highway that <unk> /><br <unk> SPOILERS <unk> /><br <unk> you already knew that , did n't <unk> /><br />Since there 's a great deal of people that apparently did not get the point of this movie , I 'd like to contribute my interpretation of why the plot makes perfect sense . As others have pointed out , one single viewing of this movie is not sufficient . If you have the DVD of <unk> , you can \" cheat \" by looking at David Lynch 's \" Top 10 <unk> to <unk> <unk> \" ( but only upon second or third viewing , please . ) ; ) < br /><br />First of all , Mulholland Drive is downright brilliant . A masterpiece . This is the kind of movie that refuse to leave your head . Not often are the comments on the DVDs very accurate , but <unk> 's \" It gets inside your head and stays there \" really hit the mark.<br /><br />David Lynch deserves praise for creating a movie that not only has a beautifully stylish look to it - cinematography - wise , has great acting ( esp . Naomi Watts ) , a haunting soundtrack by <unk> , and a very dream - like quality to it -- but on top of it all it also manages to involve the viewer in such a way that few movies have before . ( After all , when is the last time you saw a movie that just would n't leave your mind and that everyone felt compelled to talk and write about , regardless of whether they liked it or hated <unk> /><br <unk> , enough about all that , it 's time to justify those <unk> /><br />Most people that have gone through some effort to try to piece the plot together will have come to the conclusion that the first half of the picture is an illusion / a dream <unk> /><br />Of course , that 's too bad for all those trying to make sense of the movie by expecting \" traditional \" methods in which the story is laid out in a timely , logic and linear manner for the viewer . But for those expecting that , I urge you to check the name of the director and come back again . ; ) < br /><br <unk> is the story of the sad demise of Diane <unk> , a wannabe - actor who is hopelessly in love with another actor , Camilla <unk> . Due to Diane 's lack of talent , she is constantly struggling to advance her career , and feels she failed to deliver on her own and her parents ' expectations . Upon realizing that Camilla will never be hers ( C. becomes engaged with Adam <unk> , the director ) , she hires a <unk> to get rid of her , and subsequently has to deal with the guilt that it <unk> /><br />The movie first starts off with what may seem as a strange opening for this kind of thriller ; which is some 50s dance / <unk> contest , in which we can see the main character Betty giving a great performance . We also see an elderly couple ( which we will see twice more throughout the movie ) together with her , and <unk> her.<br /><br />No , wait . This is what most people see the first time they view it . There 's actually another very significant fact that is given before the credits - the camera moving into an object ( although blurry ) and the scene quickly fading out . If you look closely , the object is actually a pillow , revealing that what follows is a dream.<br /><br />The main characters seen in the first half of the <unk> /><br <unk> : Diane <unk> 's imaginary self , used in the first half of the movie that constitutes the \" dream - sequence \" - a positive portrayal of a successful , aspiring young actor ( the complete opposite of Diane ) . ' Betty ' was chosen as the name as that is the real name of the waitress at <unk> . Notice that in the dream version , the <unk> ' name is ' <unk> /><br <unk> : The fantasy version of Camilla Rhodes that , through Diane 's dream , and with the help of an imaginary car - accident , is turned into an <unk> . This makes her vulnerable and dependent on Diane 's love . She is then conveniently placed in Betty / Diane 's aunt 's luxurious home which Betty has been allowed to stay in.<br /><br <unk> : In real life , Adam 's mother . In the dream part , the woman in charge of the apartment complex that Betty stays in . She 's mainly a strong authority figure , as can be witnessed in both parts of the film.<br /><br <unk> : The director . We know from the second half that he gets engaged with Camilla . His sole purpose for being in the first half of the movie is only to serve as a punching bag for Betty / Diane , since she develops such hatred towards him.<br /><br <unk> Ruth : Diane 's real aunt , but instead of being out of town , she is actually dead . Diane inherited the money left by her aunt and used that to pay for Camilla 's murder.<br /><br />Mr . Roach : A typical Lynchian character . Not real ; appears only in Diane 's dream sequence . He 's a mysterious , influential person that controls the chain of events in the dream from his wheelchair . He serves much of the same function as the backwards - talking dwarf ( which he also plays ) in Twin <unk> /><br />The <unk> : The person that murders Camilla . This character is basically the same in both parts of the movie , although rendered in a slightly more goofy fashion in the dream sequence ( more on that <unk> /><br />Now , having established the various versions of the characters in the movie , we can begin to delve into the plot . Of course I will not go into every little detail ( neither will I lay it out chronologically ) , but I will try to explain some of the important scenes , in relation to Lynch ' \" hint - <unk> /><br />As I mentioned above , Camilla was re - produced as an <unk> through her improbable survival of a car - accident in the first 10 minutes of the movie , which left her completely vulnerable . What I found very intriguing with <unk> , is that Lynch constantly gives hints on what is real and what is n't . I 've already mentioned the camera moving into the pillow , but notice how there 's two cars riding in each lane approaching the <unk> /><br />Only one of the cars actually hit the limo ; what about the other ? Even if they stayed clear of the accident themselves , would n't they try to help the others , or at least call for help ? My theory is that , since this is a dream , the presence of the other car is just set aside , and forgotten about . Since , as <unk> Ebert so eloquently puts it \" Like real dreams , it does not explain , does not complete its sequences , lingers over what it finds fascinating , dismisses <unk> plotlines . \"<br /><br <unk> after Rita crawls down from the crash site at Mulholland Dr. , and makes her way down the <unk> and sneaks into Aunt Ruth 's apartment , Betty arrives and we see this creepy old couple driving away , staring <unk> at each other and grinning at themselves and the camera . This is the first indication that what we 're seeing is a <unk> /><br />Although the old couple seem to be unfamiliar to Betty , I think they 're actually her parents ( since they were <unk> her at the <unk> contest ) . Perhaps she did n't know them all that well , and did n't really have as good a relationship with them as she wanted , so the couple is shown as very pleasant and helpful to her in the dream . They also represent her feelings of guilt from the murder , and Diane 's sense of <unk> regarding her <unk> goals in her life.<br /><br />A rather long and hilarious scene is the one involving the <unk> . Diane apparently sees him as the major force behind the campaign trying to pressure the director to accept Camilla 's part in the movie ( from Adam 's party in the second half of the movie ) , and he therefore occupies a major part of her dream . Because of her feelings of guilt and remorse towards the murder of Camilla , a part of her wants him to miss , so she turns him into a dumb <unk> /><br />This scene , I think , is also Lynch 's attempt at totally screwing his audience over , since they 're given a false <unk> in which to view the movie.<br /><br <unk> love that ' Something just bit me bad ' line , though . : ) < br /><br />The next interesting scene is the one with the two persons at <unk> , who are having a conversation about how one of them keep having this recurring nightmare involving a man which is seen by him through a wall outside of the diner that they 're sitting in . After a little talk , they head outside and keep walking toward the corner of a fence , accompanied of course by excellent music matching the mood of the scene.<br /><br />When reaching the corner , a bum - like character with a disfigured face appears out from behind the corner , scaring the living crap out of the man having the nightmare . This nightmare exists only in Diane 's mind ; she saw that guy in the diner when paying for the murder . So , in short , her <unk> translate into that poor guy 's nightmares . The bum also <unk> Diane 's evil side , as can be witnessed later in the movie.<br /><br />The Cowboy constitutes ( along with the dwarf ) one of the strange characters that are always present in the Lynchian landscape -- Diane only saw him for a short while at Adam 's party , but just like our own dreams can award insignificant persons that we hardly know a major part in our dreams , so can he be awarded an important part in her dream . We are also given further clues during his scenes that what we 're seeing is not real ( his sudden disappearance , <unk> /><br />The Cowboy is also used as a tool to mock the Director , when he meets up with him at the odd location ( the lights here give a clear indication that this is part of a dream ) . Also notice how he says that he will appear one more time if he ( Adam ) does good , or two more times if he does bad . Throughout the movie he appears two more times , indicating to Diane that she did bad . He is also the one to wake her up to reality ( that scene is probably an illusion made to fit into her requirements of him appearing twice ) , and shortly thereafter she commits <unk> /><br />The <unk> - scene with the <unk> brothers ( where we can see <unk> , the composer , as Luigi ) is probably a result of the fact that Diane was having an <unk> just before Camilla and Adam made their announcement at Adam 's party in the second half . It could at the same time also be a statement from <unk> /><br />During the scene in which they enter Diane 's apartment , the body lying in the bed is Camilla , but notice how she 's assumed Diane 's sleeping position ; Diane is seeing herself in her own dream , but the face is not hers , although it had the same wounds on the face as Diane would have after shooting herself . This scene is also filled with some genuine Lynchian creepiness . Since Diane did not know where ( or when ) the <unk> would get to Camilla and finish her off , she just put her into her own home.<br /><br />In real life , Diane 's audition for the movie part was bad . In her dream , she delivers a perfect audition - leaving the whole crew ecstatic about her performance.<br /><br />Also interesting is the fact that the money that in real - life was used to pay for Camilla 's murder now appears in Rita / Camilla 's purse . This is part of Diane 's undoing of her terrible act by effectively being given the money back , as the murder now has n't taken place.<br /><br />When her neighbor arrives to get her piano - shaped <unk> , another hint is given ; she takes the <unk> from her table and leaves , yet later when Camilla and Betty have their encounter on the couch , we see the <unk> appear again when the camera pans over the table , suggesting that Betty 's encounter with the neighbor was a fantasy.<br /><br />The catch phrase of the movie Adam is auditioning actresses for is \" She is the girl \" ; which are the exact same words that Diane uses when giving the <unk> Camilla 's photo <unk> /><br />The blue box and the key represent the major turning point in the movie , and is where the true identities of the characters are revealed . There 's much symbolism going on here ; the box may represent Diane 's future ( it 's empty ) , or it may be a sort of a <unk> 's box ( the <unk> laughs when she asks him what the key will open ) . Either way , it is connected to the murder by means of the blue key ( which is placed next to her after the murder has taken place ) . The box is also seen at the end of the movie in the hands of the disfigured <unk> /><br <unk> <unk> is a neat little addition to further remind the viewer that what s / he is viewing is not real . It also <unk> that Diane is about to wake up to her reality ( her reality being a nightmare that she is unable to escape from , even in her <unk> /><br />During the chilling scene at the end where the creepy old couple <unk> , Diane is tormented in such a way that she sees suicide as the only way out in order to escape the screams and to avoid being haunted by her <unk> /><br />Anyway , that is my $ <unk> . Hope this could help people from bashing out at this movie and calling it ' the worst movie ever ' or something to that effect , without realizing the plot.<br /><br />As usual , Lynch is all about creating irrational fears , and he certainly achieves that with this picture as well.<br /><br />10 out of 10 .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHytOsiSUdeS",
        "colab_type": "text"
      },
      "source": [
        "### ミニバッチの中身を見てみる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWW1np1P6OQg",
        "colab_type": "text"
      },
      "source": [
        "上記のループを抜けたあとには、変数batchにはテストセットの最後のミニバッチが代入されている。\n",
        "\n",
        "そこでこの最後のミニバッチのshapeを確認する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d78vJW616H7m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "39d9583a-b288-4d8c-bebf-c428e757ab6f"
      },
      "source": [
        "batch.text.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 2640])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtDXRKPMT9KW",
        "colab_type": "text"
      },
      "source": [
        "最後の文書の末尾は「1」で埋められていることが分かる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcdyIhK0TUac",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "50155b52-d024-41bd-dc1f-33a5760862e8"
      },
      "source": [
        "print(batch.text[BATCH_SIZE-1])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([  14,   25, 2884,  ...,    1,    1,    1], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDzk2ghCUD8N",
        "colab_type": "text"
      },
      "source": [
        "ミニバッチに含まれる文書の長さを調べると、文書が文書長の降順に並べられていることが分かる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PutP_EU4Tca-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "828c5956-84e2-4bf8-c50f-dee70ac5a4e1"
      },
      "source": [
        "(batch.text != 1).sum(1)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2640, 2537, 2534, 1915, 1635, 1364, 1324, 1290, 1287, 1284, 1279, 1274,\n",
              "        1264, 1255, 1253, 1241, 1241, 1232, 1229, 1226, 1225, 1221, 1213, 1212,\n",
              "        1211, 1210, 1206, 1205, 1198, 1196, 1195, 1193, 1193, 1193, 1191, 1190,\n",
              "        1190, 1187, 1185, 1183, 1182, 1181, 1181, 1181, 1181, 1179, 1179, 1177,\n",
              "        1176, 1175, 1175, 1175, 1173, 1173, 1173, 1172, 1172, 1172, 1171, 1170,\n",
              "        1169, 1169, 1168, 1167, 1166, 1166, 1164, 1164, 1163, 1163, 1162, 1161,\n",
              "        1159, 1158, 1158, 1157, 1157, 1155, 1155, 1153, 1152, 1152, 1151, 1150,\n",
              "        1149, 1148, 1147, 1146, 1145, 1144, 1144, 1144, 1144, 1144, 1144, 1143,\n",
              "        1143, 1142, 1142, 1142], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PDZlF0O6doP",
        "colab_type": "text"
      },
      "source": [
        "## 07-02 MLPによる文書分類の準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjpel2i46gbD",
        "colab_type": "text"
      },
      "source": [
        "### 定数の設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQPXVLC66NUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "NUM_CLASS = len(LABEL.vocab)\n",
        "EMBED_DIM = 100\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "TRAIN_SIZE = len(train_data)\n",
        "DEV_SIZE = len(dev_data)\n",
        "TEST_SIZE = len(test_data)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsuHjuNp6tvt",
        "colab_type": "text"
      },
      "source": [
        "### モデルを定義する前にPyTorchの単語埋め込みがどんなものかを見てみる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3J7TzxFVMsR",
        "colab_type": "text"
      },
      "source": [
        "以下のように、語彙サイズと埋め込み次元数を指定しつつ、torch.nn.Embeddingのインスタンスを作ればよい。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP7jJVYT6tBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed = nn.Embedding(INPUT_DIM, EMBED_DIM, padding_idx=PAD_IDX)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUl6lR8JVWTu",
        "colab_type": "text"
      },
      "source": [
        "パディング用の単語の埋め込みはゼロベクトルになる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3ZCr9Ll61m8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0202fa96-2308-42e0-a7d3-738eb75602f8"
      },
      "source": [
        "print(embed(torch.tensor([[0,2,1],[2,3,4]])))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.3374, -0.1778, -0.3035, -0.5880,  0.3486,  0.6603, -0.2196,\n",
            "          -0.3792,  0.7671, -1.1925,  0.6984, -1.4097,  0.1794,  1.8951,\n",
            "           0.4954,  0.2692, -0.0770, -1.0205, -0.1690,  0.9178,  1.5810,\n",
            "           1.3010,  1.2753, -0.2010,  0.4965, -1.5723,  0.9666, -1.1481,\n",
            "          -1.1589,  0.3255, -0.6315, -2.8400, -1.3250,  0.1784, -2.1338,\n",
            "           1.0524, -0.3885, -0.9343, -0.4991, -1.0867,  0.8805,  1.5542,\n",
            "           0.6266, -0.1755,  0.0983, -0.0935,  0.2662, -0.5850,  0.8768,\n",
            "           1.6221, -1.4779,  1.1331, -1.2203,  1.3139,  1.0533,  0.1388,\n",
            "           2.2473, -0.8036, -0.2808,  0.7697, -0.6596, -0.7979,  0.1838,\n",
            "           0.2293,  0.5146,  0.9938, -0.2587, -1.0826, -0.0444,  1.6236,\n",
            "          -2.3229,  1.0878,  0.6716,  0.6933, -0.9487, -0.0765, -0.1526,\n",
            "           0.1167,  0.4403, -1.4465,  0.2553, -0.5496,  1.0042,  0.8272,\n",
            "          -0.3948,  0.4892, -0.2168, -1.7472, -1.6025, -1.0764,  0.9031,\n",
            "          -0.7218, -0.5951, -0.7112,  0.6230, -1.3729, -2.2150, -1.3193,\n",
            "          -2.0915,  0.9629],\n",
            "         [ 1.1721, -0.4372, -0.4053,  0.7086,  0.9533, -0.0130, -0.1301,\n",
            "          -0.0877, -0.0673,  0.2467, -0.9392, -1.0448,  1.2783,  0.4190,\n",
            "          -0.5073, -0.6062, -1.0532,  1.8386, -0.1095, -0.3316,  0.9008,\n",
            "           0.4840, -1.3237,  0.7869,  1.3818, -0.0694, -0.7612,  0.2416,\n",
            "          -0.5878, -1.1506,  1.0164,  0.1234,  1.1311, -0.0858, -0.0597,\n",
            "           0.3553, -1.4355,  0.0727,  0.1053, -1.0311,  1.3113, -0.0360,\n",
            "           0.2118, -0.0086,  1.8576,  2.1321, -0.5056, -0.7988, -1.0944,\n",
            "          -1.0197, -0.5399,  1.2117, -0.8632,  1.3337,  0.0771, -0.0522,\n",
            "           0.2386,  0.1411, -1.3354, -2.9340,  0.1141, -1.2072, -0.3008,\n",
            "           0.1427, -1.3027, -0.4919, -2.1429,  0.9488, -0.5684, -0.0646,\n",
            "           0.6647, -2.7836,  1.1366,  0.9089,  0.9494,  0.0266, -0.9221,\n",
            "           0.7034, -0.3659, -0.1965, -0.9207,  0.3154, -0.0217,  0.3441,\n",
            "           0.2271, -0.4597, -0.6183,  0.2461, -0.4055, -0.8368,  1.2277,\n",
            "          -0.4297, -2.2121, -0.3780,  0.9838, -1.0895,  0.2017,  0.0221,\n",
            "          -1.7753, -0.7490],\n",
            "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000]],\n",
            "\n",
            "        [[ 1.1721, -0.4372, -0.4053,  0.7086,  0.9533, -0.0130, -0.1301,\n",
            "          -0.0877, -0.0673,  0.2467, -0.9392, -1.0448,  1.2783,  0.4190,\n",
            "          -0.5073, -0.6062, -1.0532,  1.8386, -0.1095, -0.3316,  0.9008,\n",
            "           0.4840, -1.3237,  0.7869,  1.3818, -0.0694, -0.7612,  0.2416,\n",
            "          -0.5878, -1.1506,  1.0164,  0.1234,  1.1311, -0.0858, -0.0597,\n",
            "           0.3553, -1.4355,  0.0727,  0.1053, -1.0311,  1.3113, -0.0360,\n",
            "           0.2118, -0.0086,  1.8576,  2.1321, -0.5056, -0.7988, -1.0944,\n",
            "          -1.0197, -0.5399,  1.2117, -0.8632,  1.3337,  0.0771, -0.0522,\n",
            "           0.2386,  0.1411, -1.3354, -2.9340,  0.1141, -1.2072, -0.3008,\n",
            "           0.1427, -1.3027, -0.4919, -2.1429,  0.9488, -0.5684, -0.0646,\n",
            "           0.6647, -2.7836,  1.1366,  0.9089,  0.9494,  0.0266, -0.9221,\n",
            "           0.7034, -0.3659, -0.1965, -0.9207,  0.3154, -0.0217,  0.3441,\n",
            "           0.2271, -0.4597, -0.6183,  0.2461, -0.4055, -0.8368,  1.2277,\n",
            "          -0.4297, -2.2121, -0.3780,  0.9838, -1.0895,  0.2017,  0.0221,\n",
            "          -1.7753, -0.7490],\n",
            "         [ 0.2781, -0.9621, -0.4223, -1.1036,  0.2473,  1.4549, -0.2835,\n",
            "          -0.3767, -0.0306, -0.0894, -0.1965, -0.9713,  0.9005, -0.2523,\n",
            "           1.0669, -0.2985,  0.8558,  1.6098, -1.1893,  1.1677,  0.3277,\n",
            "          -0.8331, -1.6179,  0.2265, -0.4382,  0.3265, -1.5786, -1.3995,\n",
            "           0.5446, -0.0830, -1.1753,  1.7825,  1.7524, -0.2135,  0.4095,\n",
            "           0.0465,  0.6367, -0.1943, -0.8614,  0.5338,  0.9376, -0.9225,\n",
            "           0.7047, -0.2722,  0.0144, -0.6411,  2.3902, -1.4256, -0.4619,\n",
            "          -1.5539, -0.3338,  0.2405,  2.1065,  0.5509, -0.2936, -1.8027,\n",
            "          -0.6933,  1.7409,  0.2698,  0.9595, -1.0253, -0.5505,  1.0264,\n",
            "          -0.5670, -0.2658, -1.1116, -1.3696, -0.6534, -1.6125, -0.2284,\n",
            "           1.8388, -0.9473,  0.1419,  0.3696, -0.0174, -0.9575, -0.8169,\n",
            "          -0.2866,  0.4343, -0.1340, -2.1467, -1.7984, -0.6822, -0.5191,\n",
            "           0.0093, -1.8110, -0.2443,  0.1327,  1.0875, -0.1029,  0.8604,\n",
            "           0.2078,  0.2027,  0.5021, -0.4063,  0.6664,  0.4765, -1.4498,\n",
            "           1.5446,  1.0394],\n",
            "         [ 2.1681,  0.4884,  0.3359, -1.2282, -0.1200,  0.4884,  1.9431,\n",
            "           0.2169, -0.4743, -0.3679, -0.2918, -1.6531,  0.7692, -1.1323,\n",
            "           2.9590,  0.8171,  0.7668,  1.3258,  0.2103,  1.7876, -1.2128,\n",
            "           0.2045,  1.1051, -0.5454,  0.1073,  0.8727, -1.2800, -0.4619,\n",
            "           1.4342, -1.2103,  1.3834,  0.0324,  0.5421,  0.8796,  0.2713,\n",
            "           1.6067, -1.0004,  0.7392, -0.4931,  0.4073, -1.0394, -0.3226,\n",
            "           0.7226,  0.2674, -0.4673,  0.6916, -1.8752,  0.3008, -0.1468,\n",
            "           1.3672,  0.7074,  0.3276,  1.0658,  1.4130, -1.2445,  0.2227,\n",
            "           0.4593, -0.3845,  0.6554, -0.1045, -1.1134,  0.5110,  0.3566,\n",
            "           1.8591, -0.9300,  1.1186,  1.7495,  2.3058,  0.3734,  0.3314,\n",
            "          -0.1871,  0.1770,  2.9641,  0.2307,  0.3228,  0.2610,  0.3219,\n",
            "           1.7745,  0.3155, -0.9364,  0.5687, -0.0959,  0.0046, -1.4321,\n",
            "          -0.1535, -0.1925, -0.3115, -0.1812, -0.8745, -0.0270,  0.5424,\n",
            "           1.3656, -0.0284, -0.7411, -0.0169,  1.7024,  0.4206,  0.9317,\n",
            "           0.9884, -0.3948]]], grad_fn=<EmbeddingBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGyngitc78hv",
        "colab_type": "text"
      },
      "source": [
        "### モデルの定義\n",
        "* 基本的にMLPだが、入り口に単語埋め込み層が挿入されている。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9asdLYng7DOu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EmbedTextSentiment(nn.Module):\n",
        "  def __init__(self, embed_dim, num_class, vocab_size, padding_idx):\n",
        "    super(EmbedTextSentiment, self).__init__()\n",
        "    self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
        "    self.fc1 = nn.Linear(embed_dim, 500)\n",
        "    self.fc2 = nn.Linear(500, 100)\n",
        "    self.fc3 = nn.Linear(100, num_class)\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "    initrange = 0.5\n",
        "    self.fc1.weight.data.uniform_(-initrange, initrange)\n",
        "    self.fc1.bias.data.zero_()\n",
        "    self.fc2.weight.data.uniform_(-initrange, initrange)\n",
        "    self.fc2.bias.data.zero_()\n",
        "    self.fc3.weight.data.uniform_(-initrange, initrange)\n",
        "    self.fc3.bias.data.zero_()\n",
        "\n",
        "  def forward(self, text):\n",
        "    x = self.embed(text)\n",
        "    x = x.mean(1) \n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foU72cB48IO9",
        "colab_type": "text"
      },
      "source": [
        "### モデルを作る\n",
        "* インスタンスをGPUに移動させている点に注意。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0BHCGAZ8F18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = EmbedTextSentiment(EMBED_DIM, NUM_CLASS, INPUT_DIM, padding_idx=PAD_IDX).to(device)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wylQOq8N8cqI",
        "colab_type": "text"
      },
      "source": [
        "### 損失関数とoptimizerとschedulerを作る\n",
        "* 損失関数をGPUに移動させている点に注意。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gw34INS78cIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilWLfu8Z8MzW",
        "colab_type": "text"
      },
      "source": [
        "### 訓練用の関数\n",
        "* 前回とほぼ同じ。\n",
        "* データのフォーマット変更に対応させただけ。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR2R4Lqh8J7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_func():\n",
        "\n",
        "  # Train the model\n",
        "  train_loss = 0\n",
        "  train_acc = 0\n",
        "  for batch in train_iterator:\n",
        "    optimizer.zero_grad()\n",
        "    text, cls = batch.text, batch.label\n",
        "    text, cls = text.to(device), cls.to(device)\n",
        "    output = model(text)\n",
        "    loss = criterion(output, cls)\n",
        "    train_loss += loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_acc += (output.argmax(1) == cls).sum().item()\n",
        "\n",
        "  # Adjust the learning rate\n",
        "  scheduler.step()\n",
        "\n",
        "  return train_loss / TRAIN_SIZE, train_acc / TRAIN_SIZE"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftuX8e1W8iRh",
        "colab_type": "text"
      },
      "source": [
        "### 評価用の関数\n",
        "* こちらも前回とほぼ同じ。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGUnsJlq8Ue3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(data_iterator):\n",
        "  loss = 0\n",
        "  acc = 0\n",
        "  for batch in data_iterator:\n",
        "    text, cls = batch.text, batch.label\n",
        "    text, cls = text.to(device), cls.to(device)\n",
        "    with torch.no_grad():\n",
        "      output = model(text)\n",
        "      loss = criterion(output, cls)\n",
        "      loss += loss.item()\n",
        "      acc += (output.argmax(1) == cls).sum().item()\n",
        "\n",
        "  return loss, acc"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8o_jDAg8osP",
        "colab_type": "text"
      },
      "source": [
        "## 07-03 分類器の訓練と評価"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJJFv4k-8mH1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6d45f963-ac65-4f2b-a143-cf4bcfa611fb"
      },
      "source": [
        "N_EPOCHS = 20\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "  start_time = time.time()\n",
        "  train_loss, train_acc = train_func()\n",
        "  dev_loss, dev_acc = test(dev_iterator)\n",
        "  dev_loss, dev_acc = dev_loss / DEV_SIZE, dev_acc / DEV_SIZE\n",
        "\n",
        "  secs = int(time.time() - start_time)\n",
        "  mins = secs / 60\n",
        "  secs = secs % 60\n",
        "\n",
        "  print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
        "  print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
        "  print(f'\\tLoss: {dev_loss:.4f}(dev)\\t|\\tAcc: {dev_acc * 100:.1f}%(dev)')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0043(train)\t|\tAcc: 79.9%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.5%(dev)\n",
            "Epoch: 2  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0016(train)\t|\tAcc: 93.9%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.4%(dev)\n",
            "Epoch: 3  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0008(train)\t|\tAcc: 97.4%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 87.0%(dev)\n",
            "Epoch: 4  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0004(train)\t|\tAcc: 98.6%(train)\n",
            "\tLoss: 0.0003(dev)\t|\tAcc: 86.7%(dev)\n",
            "Epoch: 5  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0002(train)\t|\tAcc: 99.4%(train)\n",
            "\tLoss: 0.0003(dev)\t|\tAcc: 86.8%(dev)\n",
            "Epoch: 6  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0001(train)\t|\tAcc: 99.7%(train)\n",
            "\tLoss: 0.0004(dev)\t|\tAcc: 87.1%(dev)\n",
            "Epoch: 7  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0001(train)\t|\tAcc: 99.8%(train)\n",
            "\tLoss: 0.0004(dev)\t|\tAcc: 86.9%(dev)\n",
            "Epoch: 8  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 99.9%(train)\n",
            "\tLoss: 0.0005(dev)\t|\tAcc: 87.0%(dev)\n",
            "Epoch: 9  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0001(train)\t|\tAcc: 99.8%(train)\n",
            "\tLoss: 0.0005(dev)\t|\tAcc: 87.3%(dev)\n",
            "Epoch: 10  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 99.9%(train)\n",
            "\tLoss: 0.0005(dev)\t|\tAcc: 87.4%(dev)\n",
            "Epoch: 11  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0005(dev)\t|\tAcc: 86.9%(dev)\n",
            "Epoch: 12  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0006(dev)\t|\tAcc: 87.4%(dev)\n",
            "Epoch: 13  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0006(dev)\t|\tAcc: 87.4%(dev)\n",
            "Epoch: 14  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0006(dev)\t|\tAcc: 87.3%(dev)\n",
            "Epoch: 15  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0006(dev)\t|\tAcc: 87.4%(dev)\n",
            "Epoch: 16  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0006(dev)\t|\tAcc: 87.3%(dev)\n",
            "Epoch: 17  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0006(dev)\t|\tAcc: 87.3%(dev)\n",
            "Epoch: 18  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0007(dev)\t|\tAcc: 87.3%(dev)\n",
            "Epoch: 19  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0007(dev)\t|\tAcc: 87.3%(dev)\n",
            "Epoch: 20  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0007(dev)\t|\tAcc: 87.3%(dev)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPux8PReWTXG",
        "colab_type": "text"
      },
      "source": [
        "## 07-04 再検討\n",
        "* 訓練データ上での分類精度が100%になってしまっている。\n",
        "* 明らかにオーバーフィッティングを起こしてしまっている。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23jMgtmoWkty",
        "colab_type": "text"
      },
      "source": [
        "### dropout\n",
        "* モデルのインスタンスを作るときにdropoutの確率を引数pで指定できるようにする。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khps3ZuBWntq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EmbedTextSentiment(nn.Module):\n",
        "  def __init__(self, embed_dim, num_class, vocab_size, padding_idx, p=0.0):\n",
        "    super(EmbedTextSentiment, self).__init__()\n",
        "    self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
        "    self.fc1 = nn.Linear(embed_dim, 500)\n",
        "    self.do1 = nn.Dropout(p=p)\n",
        "    self.fc2 = nn.Linear(500, 100)\n",
        "    self.do2 = nn.Dropout(p=p)\n",
        "    self.fc3 = nn.Linear(100, num_class)\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "    initrange = 0.5\n",
        "    self.fc1.weight.data.uniform_(-initrange, initrange)\n",
        "    self.fc1.bias.data.zero_()\n",
        "    self.fc2.weight.data.uniform_(-initrange, initrange)\n",
        "    self.fc2.bias.data.zero_()\n",
        "    self.fc3.weight.data.uniform_(-initrange, initrange)\n",
        "    self.fc3.bias.data.zero_()\n",
        "\n",
        "  def forward(self, text):\n",
        "    x = self.embed(text)\n",
        "    x = x.mean(1)\n",
        "    x = F.relu(self.do1(self.fc1(x)))\n",
        "    x = F.relu(self.do2(self.fc2(x)))\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVXbkt6qXxNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = EmbedTextSentiment(EMBED_DIM, NUM_CLASS, INPUT_DIM, padding_idx=PAD_IDX, p=0.3).to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXkBDXc6X1mp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "38120bef-2e52-42dd-94ad-2a4dbcea5c2e"
      },
      "source": [
        "N_EPOCHS = 20\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "  start_time = time.time()\n",
        "  model.train()\n",
        "  train_loss, train_acc = train_func()\n",
        "  model.eval()\n",
        "  dev_loss, dev_acc = test(dev_iterator)\n",
        "  dev_loss, dev_acc = dev_loss / DEV_SIZE, dev_acc / DEV_SIZE\n",
        "\n",
        "  secs = int(time.time() - start_time)\n",
        "  mins = secs / 60\n",
        "  secs = secs % 60\n",
        "\n",
        "  print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
        "  print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
        "  print(f'\\tLoss: {dev_loss:.4f}(dev)\\t|\\tAcc: {dev_acc * 100:.1f}%(dev)')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0048(train)\t|\tAcc: 76.6%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.2%(dev)\n",
            "Epoch: 2  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0020(train)\t|\tAcc: 92.7%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.7%(dev)\n",
            "Epoch: 3  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0010(train)\t|\tAcc: 96.6%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 88.0%(dev)\n",
            "Epoch: 4  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0006(train)\t|\tAcc: 98.1%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 87.9%(dev)\n",
            "Epoch: 5  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0003(train)\t|\tAcc: 99.0%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 87.6%(dev)\n",
            "Epoch: 6  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0002(train)\t|\tAcc: 99.3%(train)\n",
            "\tLoss: 0.0003(dev)\t|\tAcc: 86.8%(dev)\n",
            "Epoch: 7  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0001(train)\t|\tAcc: 99.8%(train)\n",
            "\tLoss: 0.0005(dev)\t|\tAcc: 87.2%(dev)\n",
            "Epoch: 8  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0001(train)\t|\tAcc: 99.7%(train)\n",
            "\tLoss: 0.0005(dev)\t|\tAcc: 87.2%(dev)\n",
            "Epoch: 9  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 99.9%(train)\n",
            "\tLoss: 0.0006(dev)\t|\tAcc: 87.2%(dev)\n",
            "Epoch: 10  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 99.9%(train)\n",
            "\tLoss: 0.0005(dev)\t|\tAcc: 87.2%(dev)\n",
            "Epoch: 11  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 99.9%(train)\n",
            "\tLoss: 0.0005(dev)\t|\tAcc: 87.2%(dev)\n",
            "Epoch: 12  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 99.9%(train)\n",
            "\tLoss: 0.0006(dev)\t|\tAcc: 87.3%(dev)\n",
            "Epoch: 13  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0007(dev)\t|\tAcc: 87.3%(dev)\n",
            "Epoch: 14  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0007(dev)\t|\tAcc: 87.2%(dev)\n",
            "Epoch: 15  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0007(dev)\t|\tAcc: 87.3%(dev)\n",
            "Epoch: 16  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0007(dev)\t|\tAcc: 87.2%(dev)\n",
            "Epoch: 17  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0008(dev)\t|\tAcc: 87.2%(dev)\n",
            "Epoch: 18  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0008(dev)\t|\tAcc: 87.2%(dev)\n",
            "Epoch: 19  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0008(dev)\t|\tAcc: 87.2%(dev)\n",
            "Epoch: 20  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0008(dev)\t|\tAcc: 87.2%(dev)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu3Y-wjwb0po",
        "colab_type": "text"
      },
      "source": [
        "### L２正則化を使う\n",
        "* optimizerのweight_decayパラメータを0より大きな値にする。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmxEuSFJazCJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = EmbedTextSentiment(EMBED_DIM, NUM_CLASS, INPUT_DIM, padding_idx=PAD_IDX).to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0Zr2S7ga3J4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "03645787-e5f6-42c9-bdf4-05d957144427"
      },
      "source": [
        "N_EPOCHS = 20\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "  start_time = time.time()\n",
        "  model.train()\n",
        "  train_loss, train_acc = train_func()\n",
        "  model.eval()\n",
        "  dev_loss, dev_acc = test(dev_iterator)\n",
        "  dev_loss, dev_acc = dev_loss / DEV_SIZE, dev_acc / DEV_SIZE\n",
        "\n",
        "  secs = int(time.time() - start_time)\n",
        "  mins = secs / 60\n",
        "  secs = secs % 60\n",
        "\n",
        "  print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
        "  print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
        "  print(f'\\tLoss: {dev_loss:.4f}(dev)\\t|\\tAcc: {dev_acc * 100:.1f}%(dev)')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0063(train)\t|\tAcc: 66.4%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 78.4%(dev)\n",
            "Epoch: 2  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0046(train)\t|\tAcc: 79.1%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 82.1%(dev)\n",
            "Epoch: 3  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0042(train)\t|\tAcc: 81.4%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 84.9%(dev)\n",
            "Epoch: 4  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0037(train)\t|\tAcc: 84.9%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 82.9%(dev)\n",
            "Epoch: 5  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0036(train)\t|\tAcc: 85.0%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 85.5%(dev)\n",
            "Epoch: 6  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0033(train)\t|\tAcc: 86.7%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 85.5%(dev)\n",
            "Epoch: 7  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0033(train)\t|\tAcc: 86.8%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.0%(dev)\n",
            "Epoch: 8  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0031(train)\t|\tAcc: 88.0%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 86.9%(dev)\n",
            "Epoch: 9  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0029(train)\t|\tAcc: 88.8%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.4%(dev)\n",
            "Epoch: 10  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0028(train)\t|\tAcc: 89.4%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.7%(dev)\n",
            "Epoch: 11  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0027(train)\t|\tAcc: 90.1%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.7%(dev)\n",
            "Epoch: 12  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0026(train)\t|\tAcc: 90.5%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.5%(dev)\n",
            "Epoch: 13  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0024(train)\t|\tAcc: 91.4%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.7%(dev)\n",
            "Epoch: 14  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0024(train)\t|\tAcc: 91.6%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.0%(dev)\n",
            "Epoch: 15  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0023(train)\t|\tAcc: 92.0%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.0%(dev)\n",
            "Epoch: 16  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0021(train)\t|\tAcc: 92.8%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.2%(dev)\n",
            "Epoch: 17  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0021(train)\t|\tAcc: 93.2%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.2%(dev)\n",
            "Epoch: 18  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0020(train)\t|\tAcc: 93.7%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.2%(dev)\n",
            "Epoch: 19  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0019(train)\t|\tAcc: 94.1%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.3%(dev)\n",
            "Epoch: 20  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0018(train)\t|\tAcc: 94.3%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.3%(dev)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIHA64UTdmBj",
        "colab_type": "text"
      },
      "source": [
        "### early stopping\n",
        "* dev setでのaccuracyが3回連続で最高値を下回ったら訓練を終えることにする。\n",
        "* early stoppingの実現については、PyTorch Lightningを使う手もある。\n",
        " * https://pytorch-lightning.readthedocs.io/en/latest/early_stopping.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0zclQnVdlVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = EmbedTextSentiment(EMBED_DIM, NUM_CLASS, INPUT_DIM, padding_idx=PAD_IDX).to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3E_I5sRc3FF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "outputId": "9ebd4679-e452-457a-85ff-83f7f6f420b9"
      },
      "source": [
        "N_EPOCHS = 30 # エポック数を少し増やしておく\n",
        "patience = 3\n",
        "early_stop_count = 0\n",
        "best_dev_acc = 0.0\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "  start_time = time.time()\n",
        "  model.train()\n",
        "  train_loss, train_acc = train_func()\n",
        "  model.eval()\n",
        "  dev_loss, dev_acc = test(dev_iterator)\n",
        "  dev_loss, dev_acc = dev_loss / DEV_SIZE, dev_acc / DEV_SIZE\n",
        "\n",
        "  secs = int(time.time() - start_time)\n",
        "  mins = secs / 60\n",
        "  secs = secs % 60\n",
        "\n",
        "  print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
        "  print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
        "  print(f'\\tLoss: {dev_loss:.4f}(dev)\\t|\\tAcc: {dev_acc * 100:.1f}%(dev)')\n",
        "  if best_dev_acc <= dev_acc:\n",
        "    best_dev_acc = dev_acc\n",
        "    early_stop_count = 0\n",
        "  else:\n",
        "    early_stop_count += 1\n",
        "    if early_stop_count == patience:\n",
        "      break"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0062(train)\t|\tAcc: 67.1%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 73.3%(dev)\n",
            "Epoch: 2  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0046(train)\t|\tAcc: 79.0%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 80.7%(dev)\n",
            "Epoch: 3  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0042(train)\t|\tAcc: 81.7%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 82.8%(dev)\n",
            "Epoch: 4  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0038(train)\t|\tAcc: 84.0%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 85.3%(dev)\n",
            "Epoch: 5  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0036(train)\t|\tAcc: 85.0%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 86.1%(dev)\n",
            "Epoch: 6  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0034(train)\t|\tAcc: 86.3%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 85.4%(dev)\n",
            "Epoch: 7  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0031(train)\t|\tAcc: 87.5%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 85.8%(dev)\n",
            "Epoch: 8  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0031(train)\t|\tAcc: 87.6%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 86.4%(dev)\n",
            "Epoch: 9  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0029(train)\t|\tAcc: 88.5%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 86.5%(dev)\n",
            "Epoch: 10  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0028(train)\t|\tAcc: 89.5%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 85.8%(dev)\n",
            "Epoch: 11  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0027(train)\t|\tAcc: 89.8%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.7%(dev)\n",
            "Epoch: 12  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0025(train)\t|\tAcc: 91.0%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.4%(dev)\n",
            "Epoch: 13  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0024(train)\t|\tAcc: 91.0%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.4%(dev)\n",
            "Epoch: 14  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.0023(train)\t|\tAcc: 92.0%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 87.0%(dev)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adAOHJ-re13B",
        "colab_type": "text"
      },
      "source": [
        "ここで戻ってweight_decayやdropoutなどをチューニングし直す。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMqQ89Cpe7jJ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRvkncN09MKk",
        "colab_type": "text"
      },
      "source": [
        "## 07-05 テストセット上で評価\n",
        "* 見つけ出したベストな設定を使って、テストセット上での最終的な評価をおこなう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39XBHTCGhlUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ベストな設定を使っての学習のやり直しのコードをここに書く"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_gHj4x38y8h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "e278ea34-0c5b-4536-f0d3-a7176db51527"
      },
      "source": [
        "print('Checking the results of test dataset...')\n",
        "model.eval()\n",
        "test_loss, test_acc = test(test_iterator)\n",
        "test_loss, test_acc = test_loss / TEST_SIZE, test_acc / TEST_SIZE\n",
        "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking the results of test dataset...\n",
            "\tLoss: 0.0000(test)\t|\tAcc: 86.4%(test)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1M_VQ1xhcWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 36,
      "outputs": []
    }
  ]
}