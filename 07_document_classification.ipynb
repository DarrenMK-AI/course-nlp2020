{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07_document_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNEKXQHtZ56SUxM4xhHb2A8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course-nlp2020/blob/master/07_document_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_0ZZ8bo1mgH",
        "colab_type": "text"
      },
      "source": [
        "# 07 単語埋め込みを使った文書分類\n",
        "* 今回は、fastTextのような学習済みの単語埋め込みは使わない。\n",
        "* 単語埋め込み自体の学習も、ネットワークの重みの学習と同時におこなう。\n",
        "* IMDbデータの準備も、PyTorchのtorchtextモジュールを使っておこなう。\n",
        "* ネットワークへの入力は、単語埋め込みを、単語の出現順どおりに並べた列にする。\n",
        "* そして、前向き計算のなかではじめて、単語埋め込みの平均をとることにする。\n",
        "* 参考資料\n",
        " * https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
        " * https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb\n",
        " * https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_puYg6Zi8x3",
        "colab_type": "text"
      },
      "source": [
        "## 07-00 Google Colabのランタイムのタイプを変更する\n",
        "* Google ColabのランタイムのタイプをGPUに変更しておこう。\n",
        " * 上のメニューの「ランタイム」→「ランタイムのタイプを変更」→「ハードウェア　アクセラレータ」から「GPU」を選択"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLEeO0fw23Xp",
        "colab_type": "text"
      },
      "source": [
        "## 07-01 torchtextを使ってIMDbデータを読み込む\n",
        "* https://torchtext.readthedocs.io/en/latest/datasets.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go7epLZe3JmF",
        "colab_type": "text"
      },
      "source": [
        "### 実験の再現性確保のための設定など\n",
        "* torch.backends.cudnn.deterministicをTrueにするのは、こうしないと、GPU上での計算が毎回同じ値を与えないため。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nSqNzof1lTJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "\n",
        "SEED = 123\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y1_GyXg22f6",
        "colab_type": "text"
      },
      "source": [
        "### torchtextのフィールド\n",
        "* TEXTフィールドと、LABELフィールドという２種類のFieldオブジェクトのインスタンスを作る。\n",
        "* TEXTフィールドは、テキストの前処理の仕方を決めておくのに使う。\n",
        " * tokenizerは、デフォルトでは単にstring型のsplitメソッドを適用するだけになる。これは高速だが、tokenizationとしては雑。\n",
        "* LABELフィールドは、ラベルの前処理に使う。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjq8oooE2uQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = data.Field(tokenize=\"spacy\")\n",
        "LABEL = data.LabelField()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtEq23GS3Vxl",
        "colab_type": "text"
      },
      "source": [
        "### IMDbデータセットを前処理しつつ読み込む\n",
        "* TEXTフィールドでspaCyのtokenizationを使うように設定したので、少し時間がかかる。\n",
        " * string型splitメソッドでtokenizeすると時間はあまりかからない。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzgVXf3G3YPI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dev_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0sltPjT3j36",
        "colab_type": "text"
      },
      "source": [
        "### 最初の文書を見てみる\n",
        "（ちなみに、`vars`関数は、モジュール、クラス、インスタンス、あるいはそれ以外の`__dict__`属性を持つオブジェクトの、`__dict__`属性を辞書として返す組み込み関数。）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "692vrq6B3gZe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6be20b4b-14f4-40b9-c4ab-7c2528e80456"
      },
      "source": [
        "print(vars(train_dev_data.examples[0]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': ['The', 'vigilante', 'has', 'long', 'held', 'a', 'fascination', 'for', 'audiences', ',', 'inasmuch', 'as', 'it', 'evokes', 'a', 'sense', 'of', 'swift', ',', 'sure', 'justice', ';', 'good', 'triumphs', 'over', 'evil', 'and', 'the', 'bad', 'guy', 'gets', 'his', 'deserts', '.', 'It', 'is', ',', 'in', 'fact', ',', 'one', 'of', 'the', 'things', 'that', 'has', 'made', 'the', 'character', 'of', 'Dirty', 'Harry', 'Callahan', '(', 'as', 'played', 'by', 'Clint', 'Eastwood', ')', 'so', 'popular', '.', 'He', 'carries', 'a', 'badge', 'and', 'works', 'within', 'the', 'law', ',', 'but', 'at', 'heart', ',', 'Harry', 'is', 'a', 'vigilante', ',', 'meting', 'out', 'justice', '`', 'his', \"'\", 'way', ',', 'which', 'often', 'puts', 'him', 'in', 'conflict', 'with', 'his', 'own', 'superiors', ',', 'as', 'well', 'as', 'the', 'criminals', 'he', \"'s\", 'pursuing', '.', 'But', 'it', \"'s\", 'what', 'draws', 'the', 'audience', ';', 'anyone', 'who', \"'s\", 'ever', 'been', 'bogged', 'down', 'in', 'bureaucratic', 'nonsense', 'of', 'one', 'kind', 'or', 'another', ',', 'delights', 'in', 'seeing', 'someone', 'cut', 'through', 'the', 'red', 'tape', 'and', 'get', 'on', 'with', 'it--', 'even', 'if', 'it', \"'s\", 'only', 'on', 'the', 'screen', '.', 'And', 'that', 'satisfaction', 'derived', 'from', 'seeing', 'justice', 'done--', 'and', 'quickly--', 'is', 'one', 'of', 'the', 'elements', 'that', 'makes', '`', 'Sudden', 'Impact', ',', \"'\", 'directed', 'by', 'and', 'starring', 'Eastwood', ',', 'so', 'successful', '.', 'In', 'this', 'one', ',', 'the', 'fourth', 'of', 'the', 'series', ',', 'while', 'working', 'a', 'homicide', ',', 'Harry', 'encounters', 'a', 'bona', 'fide', 'vigilante', 'at', 'work--', 'an', 'individual', 'whose', 'brand', 'of', 'justice', 'parallels', 'his', 'own', ',', 'with', 'one', 'exception', ':', 'Whoever', 'it', 'is', ',', 'he', \"'s\", 'definitely', 'not', 'carrying', 'a', 'badge.<br', '/><br', '/>In', 'his', 'own', 'inimitable', 'way', ',', 'Inspector', 'Callahan', 'has', 'once', 'again', 'ended', 'up', 'on', 'the', 'bad', 'side', 'of', 'the', 'department', 'and', 'is', 'ordered', 'to', 'take', 'some', 'vacation', 'time', '.', 'So', 'he', 'does', ';', 'as', 'only', '`', 'Dirty', 'Harry', \"'\", 'can', '.', 'In', 'a', 'small', 'town', 'north', 'of', 'San', 'Francisco', ',', 'Harry', 'finds', 'himself', 'smack', 'dab', 'in', 'the', 'middle', 'of', 'a', 'homicide', 'case', ',', 'which', 'he', 'quickly', 'links', 'to', 'a', 'recent', 'murder', 'in', 'San', 'Francisco', 'because', 'of', 'the', 'unique', 'M.O.', 'employed', 'by', 'the', 'perpetrator', '.', 'Unaccountably', ',', 'Harry', 'encounters', 'resistance', 'from', 'the', 'local', 'Police', 'Chief', ',', 'Jannings', '(', 'Pat', 'Hingle', ')', ',', 'who', 'advises', 'him', 'to', 'take', 'his', 'big', 'city', 'tactics', 'and', 'methods', 'elsewhere', '.', 'Not', 'one', 'to', 'be', 'deterred', ',', 'however', ',', 'Harry', 'continues', 'his', 'investigation', ',', 'which', 'ultimately', 'involves', 'a', 'beautiful', 'and', 'talented', 'young', 'artist', ',', 'Jennifer', 'Spencer', '(', 'Sondra', 'Locke', ')', '.', 'Gradually', ',', 'Harry', 'discovers', 'a', 'link', 'between', 'the', 'victims', ';', 'the', 'burning', 'question', ',', 'though', ',', 'is', 'where', 'does', 'Jennifer', 'Spencer', 'fit', 'into', 'the', 'picture?<br', '/><br', '/>Eastwood', 'is', 'in', 'top', 'form', 'here', ',', 'both', 'in', 'front', 'of', 'and', 'behind', 'the', 'camera', ',', 'and', 'it', 'is', 'arguably', 'the', 'second', 'best', 'of', 'the', 'five', '-', 'film', 'series', ',', 'right', 'behind', 'the', 'original', '`', 'Dirty', 'Harry', '.', \"'\", 'It', 'had', 'been', 'seven', 'years', 'since', 'the', 'last', '`', 'Harry', \"'\", 'offering', '(', '`', 'The', 'Enforcer', ',', \"'\", '1976', ')', ',', 'but', 'Eastwood', 'steps', 'right', 'back', 'into', 'the', 'character', 'with', 'facility', 'and', 'renewed', 'vigor', '.', 'And', 'this', 'one', 'definitely', 'benefits', 'from', 'having', 'him', 'in', 'the', 'director', \"'s\", 'chair', ',', 'as', 'he', 'is', 'able', 'to', 'recapture', 'the', 'essence', 'of', ',', 'not', 'only', 'his', 'own', 'character', ',', 'but', 'that', '`', 'spirit', \"'\", 'that', 'made', 'these', 'films', 'so', 'successful', ',', 'and', 'he', 'does', 'it', 'by', 'knowing', 'the', 'territory', 'and', 'establishing', 'a', 'continuity', 'that', 'all', 'but', 'erases', 'that', 'seven', 'year', 'gap', 'between', '#', 's', '3', 'and', '4', '.', 'As', 'with', 'all', 'the', 'films', 'he', 'directs', ',', 'Eastwood', 'sets', 'a', 'deliberate', 'pace', 'that', 'works', 'perfectly', 'for', 'this', 'material', 'and', 'creates', 'just', 'enough', 'tension', 'to', 'keep', 'it', 'interesting', 'and', 'involving', 'from', 'beginning', 'to', 'end', '.', '<', 'br', '/><br', '/>The', 'screenplay', ',', 'by', 'Joseph', 'Stinson', ',', 'is', 'well', 'written', 'and', 'formulated', 'to', 'that', 'distinctive', '`', 'Dirty', 'Harry', \"'\", 'style', ';', 'the', 'dialogue', 'is', 'snappy', 'and', 'the', 'story', 'itself', '(', 'conceived', 'by', 'Charles', 'B.', 'Pierce', 'and', 'Earl', 'E.', 'Smith', ')', 'is', 'the', 'most', 'engaging', 'since', 'the', 'original', '`', 'Dirty', 'Harry', ',', \"'\", 'as', 'it', 'successfully', 'endeavors', 'to', 'play', 'upon', 'the', 'very', 'personal', 'aspects', 'of', 'the', 'drama', ',', 'rather', 'than', 'entirely', 'upon', 'the', 'action', '.', 'The', 'characters', 'are', 'well', 'drawn', 'and', 'convincing', ',', 'and', ',', 'of', 'course', ',', 'this', 'is', 'the', 'film', 'that', 'gave', 'us', 'one', 'of', 'Harry', \"'s\", 'best', 'catch', '-', 'phrases', ':', '`', 'Go', ',', 'ahead--', 'make', 'my', 'day', '...', \"'<br\", '/><br', '/>As', 'Harry', ',', 'Clint', 'Eastwood', 'perfectly', 'embodies', 'all', 'of', 'the', 'elements', 'that', 'make', 'this', 'character', 'so', 'popular', ':', 'He', 'lives', 'by', 'a', 'personal', 'moral', 'code', ',', 'a', 'true', 'individual', 'made', 'of', 'the', 'kind', 'of', 'stuff', 'we', 'envision', 'as', 'that', 'of', 'the', 'pioneers', 'who', 'settled', 'this', 'country', 'and', 'made', 'America', 'what', 'it', 'is', 'today', '.', 'Harry', 'personifies', 'that', 'sense', 'of', 'freedom', 'and', 'justice', 'we', 'all', 'strive', 'for', 'and', 'hold', 'so', 'dear', ',', 'possibly', 'more', 'so', 'today', 'than', 'ever', 'before', '.', 'No', 'matter', 'who', 'we', 'are', 'or', 'where', 'we', 'come', 'from', ',', 'there', \"'s\", 'undeniably', 'a', 'part', 'of', 'us', 'that', 'wants', 'to', 'be', 'Harry', ',', 'or', 'at', 'least', 'have', 'him', 'around', '.', '`', 'Dirty', 'Harry', \"'\", 'is', 'an', 'icon', 'of', 'the', 'cinema', ',', 'and', 'it', \"'s\", 'impossible', 'to', 'envision', 'anyone', 'but', 'Eastwood', 'portraying', 'him', ';', 'for', 'better', 'or', 'worse', ',', 'Eastwood', '`', 'is', \"'\", 'Dirty', 'Harry', ',', 'without', 'question', ',', 'just', 'as', 'Sean', 'Connery', 'is', 'James', 'Bond', 'and', 'Basil', 'Rathbone', ',', 'Sherlock', 'Holmes.<br', '/><br', '/>Sondra', 'Locke', 'is', 'entirely', 'effective', 'here', 'in', 'the', 'role', 'of', 'Jennifer', 'Spencer', ',', 'a', 'young', 'woman', 'wronged', 'and', 'out', 'for', 'vengeance', ',', 'or', 'as', 'she', 'sees', 'it', ',', '`', 'justice', '.', \"'\", 'She', 'manages', 'to', 'bring', 'a', 'hard', '-', 'edged', 'determination', 'laced', 'with', 'vulnerability', 'to', 'her', 'character', ',', 'with', 'a', 'convincing', ',', 'introspective', 'approach', 'that', 'is', 'far', 'beyond', 'what', 'is', 'typical', 'of', 'the', '`', 'action', \"'\", 'genre', '.', 'Even', 'amid', 'the', 'violence', ',', 'Locke', 'keeps', 'her', 'focus', 'on', 'Jennifer', 'and', 'the', 'traumatic', 'events', 'that', 'have', 'brought', 'her', 'to', 'this', 'stage', 'of', 'her', 'life', '.', 'Her', 'portrayal', 'makes', 'a', 'perfect', 'complement', 'to', 'Eastwood', \"'s\", 'Harry', ',', 'and', 'becomes', ',', 'in', 'philosophy', 'and', 'deed', ',', 'something', 'of', 'his', 'counterpart.<br', '/><br', '/>In', 'supporting', 'roles', ',', 'two', 'performances', 'stand', 'out', ':', 'Paul', 'Drake', ',', 'as', 'Mick', ',', 'creates', 'the', 'best', '`', 'psycho', \"'\", 'since', 'Andy', 'Robinson', \"'s\", 'dynamic', 'portrayal', 'of', 'the', 'serial', 'killer', 'in', 'the', 'original', '`', 'Dirty', 'Harry', '.', \"'\", 'With', 'actually', 'very', 'limited', 'screen', 'time', ',', 'Drake', 'establishes', 'a', 'genuinely', 'disconcerting', 'presence', 'that', 'is', 'believable', 'and', 'convincing', ',', 'which', 'adds', 'much', 'to', 'the', 'purely', 'visceral', 'response', 'of', 'the', 'audience', '.', 'This', 'is', 'the', 'guy', 'you', 'ca', \"n't\", 'wait', 'to', 'see', 'Harry', 'take', 'care', 'of', 'in', 'the', 'end', '.', 'Also', 'effective', 'is', 'Audrie', 'J.', 'Neenan', ',', 'who', 'makes', 'her', 'character', ',', 'Ray', 'Parkins', ',', 'the', 'epitome', 'of', 'the', 'proverbial', '`', 'low', 'life', ',', \"'\", 'who', 'can', 'be', 'found', 'in', 'any', 'bar', 'in', 'any', 'city', '.', 'It', \"'s\", 'a', 'performance', 'that', 'evokes', 'a', 'gut', '-', 'level', 'response', ',', 'and', 'it', 'adds', 'greatly', 'to', 'the', 'credibility', 'of', 'the', 'film', ',', 'in', 'that', 'it', 'helps', 'provide', 'that', 'necessary', 'sense', 'of', 'realism.<br', '/><br', '/>The', 'supporting', 'cast', 'includes', 'Albert', 'Popwell', '(', 'Horace', ')', ',', 'Mark', 'Kevloun', '(', 'Bennett', ')', 'and', 'Nancy', 'Parsons', '(', 'Mrs.', 'Kruger', ')', '.', 'With', 'a', 'perfect', 'blend', 'of', 'drama', 'and', 'action', ',', '`', 'Sudden', 'Impact', \"'\", 'dispenses', 'justice', 'that', 'is', 'a', 'fulfilling', 'respite', 'from', 'reality', ';', 'the', 'perfect', 'justice', 'of', 'a', 'not', '-', 'so', '-', 'perfect', 'world', ',', 'that', 'makes', 'for', 'a', 'satisfying', 'cinematic', 'experience', '.', '9/10', '.'], 'label': 'pos'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrXwYMVH3orf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "33f239d1-2090-4e1e-8c43-ac1658665555"
      },
      "source": [
        "print(train_dev_data.examples[0].text)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'vigilante', 'has', 'long', 'held', 'a', 'fascination', 'for', 'audiences', ',', 'inasmuch', 'as', 'it', 'evokes', 'a', 'sense', 'of', 'swift', ',', 'sure', 'justice', ';', 'good', 'triumphs', 'over', 'evil', 'and', 'the', 'bad', 'guy', 'gets', 'his', 'deserts', '.', 'It', 'is', ',', 'in', 'fact', ',', 'one', 'of', 'the', 'things', 'that', 'has', 'made', 'the', 'character', 'of', 'Dirty', 'Harry', 'Callahan', '(', 'as', 'played', 'by', 'Clint', 'Eastwood', ')', 'so', 'popular', '.', 'He', 'carries', 'a', 'badge', 'and', 'works', 'within', 'the', 'law', ',', 'but', 'at', 'heart', ',', 'Harry', 'is', 'a', 'vigilante', ',', 'meting', 'out', 'justice', '`', 'his', \"'\", 'way', ',', 'which', 'often', 'puts', 'him', 'in', 'conflict', 'with', 'his', 'own', 'superiors', ',', 'as', 'well', 'as', 'the', 'criminals', 'he', \"'s\", 'pursuing', '.', 'But', 'it', \"'s\", 'what', 'draws', 'the', 'audience', ';', 'anyone', 'who', \"'s\", 'ever', 'been', 'bogged', 'down', 'in', 'bureaucratic', 'nonsense', 'of', 'one', 'kind', 'or', 'another', ',', 'delights', 'in', 'seeing', 'someone', 'cut', 'through', 'the', 'red', 'tape', 'and', 'get', 'on', 'with', 'it--', 'even', 'if', 'it', \"'s\", 'only', 'on', 'the', 'screen', '.', 'And', 'that', 'satisfaction', 'derived', 'from', 'seeing', 'justice', 'done--', 'and', 'quickly--', 'is', 'one', 'of', 'the', 'elements', 'that', 'makes', '`', 'Sudden', 'Impact', ',', \"'\", 'directed', 'by', 'and', 'starring', 'Eastwood', ',', 'so', 'successful', '.', 'In', 'this', 'one', ',', 'the', 'fourth', 'of', 'the', 'series', ',', 'while', 'working', 'a', 'homicide', ',', 'Harry', 'encounters', 'a', 'bona', 'fide', 'vigilante', 'at', 'work--', 'an', 'individual', 'whose', 'brand', 'of', 'justice', 'parallels', 'his', 'own', ',', 'with', 'one', 'exception', ':', 'Whoever', 'it', 'is', ',', 'he', \"'s\", 'definitely', 'not', 'carrying', 'a', 'badge.<br', '/><br', '/>In', 'his', 'own', 'inimitable', 'way', ',', 'Inspector', 'Callahan', 'has', 'once', 'again', 'ended', 'up', 'on', 'the', 'bad', 'side', 'of', 'the', 'department', 'and', 'is', 'ordered', 'to', 'take', 'some', 'vacation', 'time', '.', 'So', 'he', 'does', ';', 'as', 'only', '`', 'Dirty', 'Harry', \"'\", 'can', '.', 'In', 'a', 'small', 'town', 'north', 'of', 'San', 'Francisco', ',', 'Harry', 'finds', 'himself', 'smack', 'dab', 'in', 'the', 'middle', 'of', 'a', 'homicide', 'case', ',', 'which', 'he', 'quickly', 'links', 'to', 'a', 'recent', 'murder', 'in', 'San', 'Francisco', 'because', 'of', 'the', 'unique', 'M.O.', 'employed', 'by', 'the', 'perpetrator', '.', 'Unaccountably', ',', 'Harry', 'encounters', 'resistance', 'from', 'the', 'local', 'Police', 'Chief', ',', 'Jannings', '(', 'Pat', 'Hingle', ')', ',', 'who', 'advises', 'him', 'to', 'take', 'his', 'big', 'city', 'tactics', 'and', 'methods', 'elsewhere', '.', 'Not', 'one', 'to', 'be', 'deterred', ',', 'however', ',', 'Harry', 'continues', 'his', 'investigation', ',', 'which', 'ultimately', 'involves', 'a', 'beautiful', 'and', 'talented', 'young', 'artist', ',', 'Jennifer', 'Spencer', '(', 'Sondra', 'Locke', ')', '.', 'Gradually', ',', 'Harry', 'discovers', 'a', 'link', 'between', 'the', 'victims', ';', 'the', 'burning', 'question', ',', 'though', ',', 'is', 'where', 'does', 'Jennifer', 'Spencer', 'fit', 'into', 'the', 'picture?<br', '/><br', '/>Eastwood', 'is', 'in', 'top', 'form', 'here', ',', 'both', 'in', 'front', 'of', 'and', 'behind', 'the', 'camera', ',', 'and', 'it', 'is', 'arguably', 'the', 'second', 'best', 'of', 'the', 'five', '-', 'film', 'series', ',', 'right', 'behind', 'the', 'original', '`', 'Dirty', 'Harry', '.', \"'\", 'It', 'had', 'been', 'seven', 'years', 'since', 'the', 'last', '`', 'Harry', \"'\", 'offering', '(', '`', 'The', 'Enforcer', ',', \"'\", '1976', ')', ',', 'but', 'Eastwood', 'steps', 'right', 'back', 'into', 'the', 'character', 'with', 'facility', 'and', 'renewed', 'vigor', '.', 'And', 'this', 'one', 'definitely', 'benefits', 'from', 'having', 'him', 'in', 'the', 'director', \"'s\", 'chair', ',', 'as', 'he', 'is', 'able', 'to', 'recapture', 'the', 'essence', 'of', ',', 'not', 'only', 'his', 'own', 'character', ',', 'but', 'that', '`', 'spirit', \"'\", 'that', 'made', 'these', 'films', 'so', 'successful', ',', 'and', 'he', 'does', 'it', 'by', 'knowing', 'the', 'territory', 'and', 'establishing', 'a', 'continuity', 'that', 'all', 'but', 'erases', 'that', 'seven', 'year', 'gap', 'between', '#', 's', '3', 'and', '4', '.', 'As', 'with', 'all', 'the', 'films', 'he', 'directs', ',', 'Eastwood', 'sets', 'a', 'deliberate', 'pace', 'that', 'works', 'perfectly', 'for', 'this', 'material', 'and', 'creates', 'just', 'enough', 'tension', 'to', 'keep', 'it', 'interesting', 'and', 'involving', 'from', 'beginning', 'to', 'end', '.', '<', 'br', '/><br', '/>The', 'screenplay', ',', 'by', 'Joseph', 'Stinson', ',', 'is', 'well', 'written', 'and', 'formulated', 'to', 'that', 'distinctive', '`', 'Dirty', 'Harry', \"'\", 'style', ';', 'the', 'dialogue', 'is', 'snappy', 'and', 'the', 'story', 'itself', '(', 'conceived', 'by', 'Charles', 'B.', 'Pierce', 'and', 'Earl', 'E.', 'Smith', ')', 'is', 'the', 'most', 'engaging', 'since', 'the', 'original', '`', 'Dirty', 'Harry', ',', \"'\", 'as', 'it', 'successfully', 'endeavors', 'to', 'play', 'upon', 'the', 'very', 'personal', 'aspects', 'of', 'the', 'drama', ',', 'rather', 'than', 'entirely', 'upon', 'the', 'action', '.', 'The', 'characters', 'are', 'well', 'drawn', 'and', 'convincing', ',', 'and', ',', 'of', 'course', ',', 'this', 'is', 'the', 'film', 'that', 'gave', 'us', 'one', 'of', 'Harry', \"'s\", 'best', 'catch', '-', 'phrases', ':', '`', 'Go', ',', 'ahead--', 'make', 'my', 'day', '...', \"'<br\", '/><br', '/>As', 'Harry', ',', 'Clint', 'Eastwood', 'perfectly', 'embodies', 'all', 'of', 'the', 'elements', 'that', 'make', 'this', 'character', 'so', 'popular', ':', 'He', 'lives', 'by', 'a', 'personal', 'moral', 'code', ',', 'a', 'true', 'individual', 'made', 'of', 'the', 'kind', 'of', 'stuff', 'we', 'envision', 'as', 'that', 'of', 'the', 'pioneers', 'who', 'settled', 'this', 'country', 'and', 'made', 'America', 'what', 'it', 'is', 'today', '.', 'Harry', 'personifies', 'that', 'sense', 'of', 'freedom', 'and', 'justice', 'we', 'all', 'strive', 'for', 'and', 'hold', 'so', 'dear', ',', 'possibly', 'more', 'so', 'today', 'than', 'ever', 'before', '.', 'No', 'matter', 'who', 'we', 'are', 'or', 'where', 'we', 'come', 'from', ',', 'there', \"'s\", 'undeniably', 'a', 'part', 'of', 'us', 'that', 'wants', 'to', 'be', 'Harry', ',', 'or', 'at', 'least', 'have', 'him', 'around', '.', '`', 'Dirty', 'Harry', \"'\", 'is', 'an', 'icon', 'of', 'the', 'cinema', ',', 'and', 'it', \"'s\", 'impossible', 'to', 'envision', 'anyone', 'but', 'Eastwood', 'portraying', 'him', ';', 'for', 'better', 'or', 'worse', ',', 'Eastwood', '`', 'is', \"'\", 'Dirty', 'Harry', ',', 'without', 'question', ',', 'just', 'as', 'Sean', 'Connery', 'is', 'James', 'Bond', 'and', 'Basil', 'Rathbone', ',', 'Sherlock', 'Holmes.<br', '/><br', '/>Sondra', 'Locke', 'is', 'entirely', 'effective', 'here', 'in', 'the', 'role', 'of', 'Jennifer', 'Spencer', ',', 'a', 'young', 'woman', 'wronged', 'and', 'out', 'for', 'vengeance', ',', 'or', 'as', 'she', 'sees', 'it', ',', '`', 'justice', '.', \"'\", 'She', 'manages', 'to', 'bring', 'a', 'hard', '-', 'edged', 'determination', 'laced', 'with', 'vulnerability', 'to', 'her', 'character', ',', 'with', 'a', 'convincing', ',', 'introspective', 'approach', 'that', 'is', 'far', 'beyond', 'what', 'is', 'typical', 'of', 'the', '`', 'action', \"'\", 'genre', '.', 'Even', 'amid', 'the', 'violence', ',', 'Locke', 'keeps', 'her', 'focus', 'on', 'Jennifer', 'and', 'the', 'traumatic', 'events', 'that', 'have', 'brought', 'her', 'to', 'this', 'stage', 'of', 'her', 'life', '.', 'Her', 'portrayal', 'makes', 'a', 'perfect', 'complement', 'to', 'Eastwood', \"'s\", 'Harry', ',', 'and', 'becomes', ',', 'in', 'philosophy', 'and', 'deed', ',', 'something', 'of', 'his', 'counterpart.<br', '/><br', '/>In', 'supporting', 'roles', ',', 'two', 'performances', 'stand', 'out', ':', 'Paul', 'Drake', ',', 'as', 'Mick', ',', 'creates', 'the', 'best', '`', 'psycho', \"'\", 'since', 'Andy', 'Robinson', \"'s\", 'dynamic', 'portrayal', 'of', 'the', 'serial', 'killer', 'in', 'the', 'original', '`', 'Dirty', 'Harry', '.', \"'\", 'With', 'actually', 'very', 'limited', 'screen', 'time', ',', 'Drake', 'establishes', 'a', 'genuinely', 'disconcerting', 'presence', 'that', 'is', 'believable', 'and', 'convincing', ',', 'which', 'adds', 'much', 'to', 'the', 'purely', 'visceral', 'response', 'of', 'the', 'audience', '.', 'This', 'is', 'the', 'guy', 'you', 'ca', \"n't\", 'wait', 'to', 'see', 'Harry', 'take', 'care', 'of', 'in', 'the', 'end', '.', 'Also', 'effective', 'is', 'Audrie', 'J.', 'Neenan', ',', 'who', 'makes', 'her', 'character', ',', 'Ray', 'Parkins', ',', 'the', 'epitome', 'of', 'the', 'proverbial', '`', 'low', 'life', ',', \"'\", 'who', 'can', 'be', 'found', 'in', 'any', 'bar', 'in', 'any', 'city', '.', 'It', \"'s\", 'a', 'performance', 'that', 'evokes', 'a', 'gut', '-', 'level', 'response', ',', 'and', 'it', 'adds', 'greatly', 'to', 'the', 'credibility', 'of', 'the', 'film', ',', 'in', 'that', 'it', 'helps', 'provide', 'that', 'necessary', 'sense', 'of', 'realism.<br', '/><br', '/>The', 'supporting', 'cast', 'includes', 'Albert', 'Popwell', '(', 'Horace', ')', ',', 'Mark', 'Kevloun', '(', 'Bennett', ')', 'and', 'Nancy', 'Parsons', '(', 'Mrs.', 'Kruger', ')', '.', 'With', 'a', 'perfect', 'blend', 'of', 'drama', 'and', 'action', ',', '`', 'Sudden', 'Impact', \"'\", 'dispenses', 'justice', 'that', 'is', 'a', 'fulfilling', 'respite', 'from', 'reality', ';', 'the', 'perfect', 'justice', 'of', 'a', 'not', '-', 'so', '-', 'perfect', 'world', ',', 'that', 'makes', 'for', 'a', 'satisfying', 'cinematic', 'experience', '.', '9/10', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRMuAOum3rB5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c533c53d-40c5-4be4-bd67-362e6fd9ebcd"
      },
      "source": [
        "print(train_dev_data.examples[0].label)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgZgQbyD3u9D",
        "colab_type": "text"
      },
      "source": [
        "### テストセット以外の部分を訓練データと検証データに分ける"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2FtnEKZ32hM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, dev_data = train_dev_data.split(split_ratio=0.8, random_state = random.seed(SEED))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fzsi9ZC36eR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "d07ea6ca-b95c-4ffd-933e-9f0d6315e90a"
      },
      "source": [
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of development examples: {len(dev_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 20000\n",
            "Number of development examples: 5000\n",
            "Number of testing examples: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oXz2lvB37Vm",
        "colab_type": "text"
      },
      "source": [
        "### データセットのラベルを作る\n",
        "* TEXTラベルのほうでは、最大語彙サイズを指定する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBQeD7yC37x4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_VOCAB_SIZE = 25000\n",
        "\n",
        "TEXT.build_vocab(train_data, max_size=MAX_VOCAB_SIZE)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iv6RSh3HmLf",
        "colab_type": "text"
      },
      "source": [
        "なぜ語彙サイズが25,000ではなく25,002なのか、については少し下の説明を参照。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWuYQthC4Ml8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "cf52a62c-f4c3-42de-b68d-30ab21cef5b0"
      },
      "source": [
        "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
        "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in TEXT vocabulary: 25002\n",
            "Unique tokens in LABEL vocabulary: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW4eR-K44Rba",
        "colab_type": "text"
      },
      "source": [
        "### 出現頻度順で上位２０単語を見てみる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jan98ffr4PXP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c65a9308-915f-40e1-c7a6-dffb7e1676d6"
      },
      "source": [
        "print(TEXT.vocab.freqs.most_common(20))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('the', 231547), (',', 220008), ('.', 189104), ('and', 125224), ('a', 124944), ('of', 115034), ('to', 107158), ('is', 87000), ('in', 70317), ('I', 62117), ('it', 61401), ('that', 56179), ('\"', 50540), (\"'s\", 49238), ('this', 48511), ('-', 42040), ('/><br', 40630), ('was', 39971), ('as', 34721), ('with', 34233)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKQojOuv4Z38",
        "colab_type": "text"
      },
      "source": [
        "### 単語ID順に最初の１０単語を見てみる\n",
        "* IDのうち、0と1は、未知語とパディング用の単語という特殊な単語に割り振られている。\n",
        " * パディングとは、長さが不揃いの複数の文書を同じミニバッチにまとめるとき、すべての文書の長さを無理やりそろえるため、文書末尾に特殊な単語（元々の語彙にない、人工的に用意した単語）を追加すること。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlhXRT3g4Xad",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e9495bf-fe76-4fd0-ba43-e9c92108255b"
      },
      "source": [
        "print(TEXT.vocab.itos[:10])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vJfHTdR4qd4",
        "colab_type": "text"
      },
      "source": [
        "### ラベルのほうのIDを確認する\n",
        "* こちらはnegとposに対応する２つのIDしかない。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI7Pz_6R4bYM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "085b6b53-1513-4cdc-a7eb-bc60aa005b64"
      },
      "source": [
        "print(LABEL.vocab.stoi)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<function _default_unk_index at 0x7fea7b6062f0>, {'neg': 0, 'pos': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14_znTjp4w5s",
        "colab_type": "text"
      },
      "source": [
        "### ミニバッチを取り出すためのiteratorを作る\n",
        "* ミニバッチのサイズを指定する。\n",
        " * ミニバッチのサイズは、性能を出すためにチューニングすべきハイパーパラメータのひとつ。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUED86Jb4tUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 100\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, dev_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, dev_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAW9Ec5q6BQO",
        "colab_type": "text"
      },
      "source": [
        "### 試しにテストセットのiteratorを回してミニバッチをすべて取得して個数を数えてみる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kpn4tfWl42kY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cf1ee808-460e-4573-e8ec-870cabd04e69"
      },
      "source": [
        "i = 0\n",
        "for batch in test_iterator:\n",
        "  i += 1\n",
        "  continue\n",
        "print(f'We have {i} mini-batches in test set.')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 250 mini-batches in test set.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHytOsiSUdeS",
        "colab_type": "text"
      },
      "source": [
        "### ミニバッチの中身を見てみる\n",
        "* 上記のループを抜けたあとには、変数batchにはテストセットの最後のミニバッチが代入されている。\n",
        "* そこで、この最後のミニバッチのshapeを確認する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWW1np1P6OQg",
        "colab_type": "text"
      },
      "source": [
        "### ミニバッチのshapeを確認する\n",
        "* ミニバッチの形は、[ミニバッチに含まれる最長文書の文書長, ミニバッチのサイズ]になっていることに注意！\n",
        " * ミニバッチのサイズが最初に来ているのではない！\n",
        "* [ミニバッチのサイズ, ミニバッチに含まれる最長文書の文書長]という形にしたいときは、テキストのfieldを作るときにに以下のようにする。\n",
        "\n",
        "__`TEXT = data.Field(tokenize=\"spacy\", batch_first=True)`__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d78vJW616H7m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "68304b18-a3f6-47cf-a888-28da7ddbc2e2"
      },
      "source": [
        "batch.text.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2640, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHMHkR73VuCD",
        "colab_type": "text"
      },
      "source": [
        "このミニバッチに含まれる文書のうち、最初のものの単語ID列と、先頭100個のIDを単語に戻したものを表示させてみる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tZLm0hQVjZE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "36940b66-82d3-4d17-ae27-5c671dbe4af6"
      },
      "source": [
        "print(batch.text[:, 0])\n",
        "print(' '.join([TEXT.vocab.itos[i] for i in batch.text[:100, 0]]))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([157,  15,   6,  ...,   7, 315,   4], device='cuda:0')\n",
            "There 's a sign on The Lost Highway that <unk> /><br <unk> SPOILERS <unk> /><br <unk> you already knew that , did n't <unk> /><br />Since there 's a great deal of people that apparently did not get the point of this movie , I 'd like to contribute my interpretation of why the plot makes perfect sense . As others have pointed out , one single viewing of this movie is not sufficient . If you have the DVD of <unk> , you can \" cheat \" by looking at David Lynch 's \" Top 10 <unk> to <unk>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtDXRKPMT9KW",
        "colab_type": "text"
      },
      "source": [
        "最後の文書の末尾は「1」で埋められていることが分かる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcdyIhK0TUac",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "18221dae-b12c-48ba-d14b-413a2e9aaebb"
      },
      "source": [
        "print(batch.text[:, BATCH_SIZE-1])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([  14,   25, 2791,  ...,    1,    1,    1], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDzk2ghCUD8N",
        "colab_type": "text"
      },
      "source": [
        "ミニバッチに含まれる文書の長さを調べると、文書が文書長の降順に並べられていることが分かる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PutP_EU4Tca-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "5d77a51d-8b25-40ff-a353-2d8b43133ae9"
      },
      "source": [
        "(batch.text != 1).sum(0)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2640, 2537, 2534, 1915, 1635, 1364, 1324, 1290, 1287, 1284, 1279, 1274,\n",
              "        1264, 1255, 1253, 1241, 1241, 1232, 1229, 1226, 1225, 1221, 1213, 1212,\n",
              "        1211, 1210, 1206, 1205, 1198, 1196, 1195, 1193, 1193, 1193, 1191, 1190,\n",
              "        1190, 1187, 1185, 1183, 1182, 1181, 1181, 1181, 1181, 1179, 1179, 1177,\n",
              "        1176, 1175, 1175, 1175, 1173, 1173, 1173, 1172, 1172, 1172, 1171, 1170,\n",
              "        1169, 1169, 1168, 1167, 1166, 1166, 1164, 1164, 1163, 1163, 1162, 1161,\n",
              "        1159, 1158, 1158, 1157, 1157, 1155, 1155, 1153, 1152, 1152, 1151, 1150,\n",
              "        1149, 1148, 1147, 1146, 1145, 1144, 1144, 1144, 1144, 1144, 1144, 1143,\n",
              "        1143, 1142, 1142, 1142], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PDZlF0O6doP",
        "colab_type": "text"
      },
      "source": [
        "## 07-02 MLPによる文書分類の準備\n",
        "* 今回は、ごく簡単なMLPで文書分類をする。\n",
        "* 文書中の全単語トークンの埋め込みベクトルの平均を、MLPの入力とする。\n",
        " * 当然、語順の情報は使われない。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjpel2i46gbD",
        "colab_type": "text"
      },
      "source": [
        "### 定数の設定\n",
        "* 単語埋め込みベクトルの次元数は100にする。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQPXVLC66NUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "NUM_CLASS = len(LABEL.vocab)\n",
        "EMBED_DIM = 100\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsuHjuNp6tvt",
        "colab_type": "text"
      },
      "source": [
        "### モデルを定義する前にPyTorchの単語埋め込みがどんなものかを見てみる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3J7TzxFVMsR",
        "colab_type": "text"
      },
      "source": [
        "以下のように、語彙サイズと埋め込みの次元数を指定しつつ、torch.nn.Embeddingのインスタンスを作ればよい。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP7jJVYT6tBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed = nn.Embedding(INPUT_DIM, EMBED_DIM, padding_idx=PAD_IDX)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUl6lR8JVWTu",
        "colab_type": "text"
      },
      "source": [
        "パディング用の単語の埋め込みはゼロベクトルになる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3ZCr9Ll61m8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "404d05fe-dd8b-4978-80f3-a34383f5adb1"
      },
      "source": [
        "print(embed(torch.tensor([[2,3,4],[0,2,1]])))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 1.1721, -0.4372, -0.4053,  0.7086,  0.9533, -0.0130, -0.1301,\n",
            "          -0.0877, -0.0673,  0.2467, -0.9392, -1.0448,  1.2783,  0.4190,\n",
            "          -0.5073, -0.6062, -1.0532,  1.8386, -0.1095, -0.3316,  0.9008,\n",
            "           0.4840, -1.3237,  0.7869,  1.3818, -0.0694, -0.7612,  0.2416,\n",
            "          -0.5878, -1.1506,  1.0164,  0.1234,  1.1311, -0.0858, -0.0597,\n",
            "           0.3553, -1.4355,  0.0727,  0.1053, -1.0311,  1.3113, -0.0360,\n",
            "           0.2118, -0.0086,  1.8576,  2.1321, -0.5056, -0.7988, -1.0944,\n",
            "          -1.0197, -0.5399,  1.2117, -0.8632,  1.3337,  0.0771, -0.0522,\n",
            "           0.2386,  0.1411, -1.3354, -2.9340,  0.1141, -1.2072, -0.3008,\n",
            "           0.1427, -1.3027, -0.4919, -2.1429,  0.9488, -0.5684, -0.0646,\n",
            "           0.6647, -2.7836,  1.1366,  0.9089,  0.9494,  0.0266, -0.9221,\n",
            "           0.7034, -0.3659, -0.1965, -0.9207,  0.3154, -0.0217,  0.3441,\n",
            "           0.2271, -0.4597, -0.6183,  0.2461, -0.4055, -0.8368,  1.2277,\n",
            "          -0.4297, -2.2121, -0.3780,  0.9838, -1.0895,  0.2017,  0.0221,\n",
            "          -1.7753, -0.7490],\n",
            "         [ 0.2781, -0.9621, -0.4223, -1.1036,  0.2473,  1.4549, -0.2835,\n",
            "          -0.3767, -0.0306, -0.0894, -0.1965, -0.9713,  0.9005, -0.2523,\n",
            "           1.0669, -0.2985,  0.8558,  1.6098, -1.1893,  1.1677,  0.3277,\n",
            "          -0.8331, -1.6179,  0.2265, -0.4382,  0.3265, -1.5786, -1.3995,\n",
            "           0.5446, -0.0830, -1.1753,  1.7825,  1.7524, -0.2135,  0.4095,\n",
            "           0.0465,  0.6367, -0.1943, -0.8614,  0.5338,  0.9376, -0.9225,\n",
            "           0.7047, -0.2722,  0.0144, -0.6411,  2.3902, -1.4256, -0.4619,\n",
            "          -1.5539, -0.3338,  0.2405,  2.1065,  0.5509, -0.2936, -1.8027,\n",
            "          -0.6933,  1.7409,  0.2698,  0.9595, -1.0253, -0.5505,  1.0264,\n",
            "          -0.5670, -0.2658, -1.1116, -1.3696, -0.6534, -1.6125, -0.2284,\n",
            "           1.8388, -0.9473,  0.1419,  0.3696, -0.0174, -0.9575, -0.8169,\n",
            "          -0.2866,  0.4343, -0.1340, -2.1467, -1.7984, -0.6822, -0.5191,\n",
            "           0.0093, -1.8110, -0.2443,  0.1327,  1.0875, -0.1029,  0.8604,\n",
            "           0.2078,  0.2027,  0.5021, -0.4063,  0.6664,  0.4765, -1.4498,\n",
            "           1.5446,  1.0394],\n",
            "         [ 2.1681,  0.4884,  0.3359, -1.2282, -0.1200,  0.4884,  1.9431,\n",
            "           0.2169, -0.4743, -0.3679, -0.2918, -1.6531,  0.7692, -1.1323,\n",
            "           2.9590,  0.8171,  0.7668,  1.3258,  0.2103,  1.7876, -1.2128,\n",
            "           0.2045,  1.1051, -0.5454,  0.1073,  0.8727, -1.2800, -0.4619,\n",
            "           1.4342, -1.2103,  1.3834,  0.0324,  0.5421,  0.8796,  0.2713,\n",
            "           1.6067, -1.0004,  0.7392, -0.4931,  0.4073, -1.0394, -0.3226,\n",
            "           0.7226,  0.2674, -0.4673,  0.6916, -1.8752,  0.3008, -0.1468,\n",
            "           1.3672,  0.7074,  0.3276,  1.0658,  1.4130, -1.2445,  0.2227,\n",
            "           0.4593, -0.3845,  0.6554, -0.1045, -1.1134,  0.5110,  0.3566,\n",
            "           1.8591, -0.9300,  1.1186,  1.7495,  2.3058,  0.3734,  0.3314,\n",
            "          -0.1871,  0.1770,  2.9641,  0.2307,  0.3228,  0.2610,  0.3219,\n",
            "           1.7745,  0.3155, -0.9364,  0.5687, -0.0959,  0.0046, -1.4321,\n",
            "          -0.1535, -0.1925, -0.3115, -0.1812, -0.8745, -0.0270,  0.5424,\n",
            "           1.3656, -0.0284, -0.7411, -0.0169,  1.7024,  0.4206,  0.9317,\n",
            "           0.9884, -0.3948]],\n",
            "\n",
            "        [[ 0.3374, -0.1778, -0.3035, -0.5880,  0.3486,  0.6603, -0.2196,\n",
            "          -0.3792,  0.7671, -1.1925,  0.6984, -1.4097,  0.1794,  1.8951,\n",
            "           0.4954,  0.2692, -0.0770, -1.0205, -0.1690,  0.9178,  1.5810,\n",
            "           1.3010,  1.2753, -0.2010,  0.4965, -1.5723,  0.9666, -1.1481,\n",
            "          -1.1589,  0.3255, -0.6315, -2.8400, -1.3250,  0.1784, -2.1338,\n",
            "           1.0524, -0.3885, -0.9343, -0.4991, -1.0867,  0.8805,  1.5542,\n",
            "           0.6266, -0.1755,  0.0983, -0.0935,  0.2662, -0.5850,  0.8768,\n",
            "           1.6221, -1.4779,  1.1331, -1.2203,  1.3139,  1.0533,  0.1388,\n",
            "           2.2473, -0.8036, -0.2808,  0.7697, -0.6596, -0.7979,  0.1838,\n",
            "           0.2293,  0.5146,  0.9938, -0.2587, -1.0826, -0.0444,  1.6236,\n",
            "          -2.3229,  1.0878,  0.6716,  0.6933, -0.9487, -0.0765, -0.1526,\n",
            "           0.1167,  0.4403, -1.4465,  0.2553, -0.5496,  1.0042,  0.8272,\n",
            "          -0.3948,  0.4892, -0.2168, -1.7472, -1.6025, -1.0764,  0.9031,\n",
            "          -0.7218, -0.5951, -0.7112,  0.6230, -1.3729, -2.2150, -1.3193,\n",
            "          -2.0915,  0.9629],\n",
            "         [ 1.1721, -0.4372, -0.4053,  0.7086,  0.9533, -0.0130, -0.1301,\n",
            "          -0.0877, -0.0673,  0.2467, -0.9392, -1.0448,  1.2783,  0.4190,\n",
            "          -0.5073, -0.6062, -1.0532,  1.8386, -0.1095, -0.3316,  0.9008,\n",
            "           0.4840, -1.3237,  0.7869,  1.3818, -0.0694, -0.7612,  0.2416,\n",
            "          -0.5878, -1.1506,  1.0164,  0.1234,  1.1311, -0.0858, -0.0597,\n",
            "           0.3553, -1.4355,  0.0727,  0.1053, -1.0311,  1.3113, -0.0360,\n",
            "           0.2118, -0.0086,  1.8576,  2.1321, -0.5056, -0.7988, -1.0944,\n",
            "          -1.0197, -0.5399,  1.2117, -0.8632,  1.3337,  0.0771, -0.0522,\n",
            "           0.2386,  0.1411, -1.3354, -2.9340,  0.1141, -1.2072, -0.3008,\n",
            "           0.1427, -1.3027, -0.4919, -2.1429,  0.9488, -0.5684, -0.0646,\n",
            "           0.6647, -2.7836,  1.1366,  0.9089,  0.9494,  0.0266, -0.9221,\n",
            "           0.7034, -0.3659, -0.1965, -0.9207,  0.3154, -0.0217,  0.3441,\n",
            "           0.2271, -0.4597, -0.6183,  0.2461, -0.4055, -0.8368,  1.2277,\n",
            "          -0.4297, -2.2121, -0.3780,  0.9838, -1.0895,  0.2017,  0.0221,\n",
            "          -1.7753, -0.7490],\n",
            "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000]]], grad_fn=<EmbeddingBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGyngitc78hv",
        "colab_type": "text"
      },
      "source": [
        "### モデルの定義\n",
        "* MLP（多層パーセプトロン）だが、入り口に単語埋め込み層が挿入されている。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9asdLYng7DOu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EmbedTextSentiment(nn.Module):\n",
        "  def __init__(self, embed_dim, num_class, vocab_size, padding_idx):\n",
        "    super(EmbedTextSentiment, self).__init__()\n",
        "    self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
        "    self.fc1 = nn.Linear(embed_dim, 500)\n",
        "    self.fc2 = nn.Linear(500, 100)\n",
        "    self.fc3 = nn.Linear(100, num_class)\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "    initrange = 0.5\n",
        "    self.fc1.weight.data.uniform_(-initrange, initrange)\n",
        "    self.fc1.bias.data.zero_()\n",
        "    self.fc2.weight.data.uniform_(-initrange, initrange)\n",
        "    self.fc2.bias.data.zero_()\n",
        "    self.fc3.weight.data.uniform_(-initrange, initrange)\n",
        "    self.fc3.bias.data.zero_()\n",
        "\n",
        "  def forward(self, text):\n",
        "    x = self.embed(text)\n",
        "    x = x.mean(0) \n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foU72cB48IO9",
        "colab_type": "text"
      },
      "source": [
        "### モデルを作る\n",
        "* モデル（のインスタンス）をGPUに移動させている点に注意。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0BHCGAZ8F18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = EmbedTextSentiment(EMBED_DIM, NUM_CLASS, INPUT_DIM, padding_idx=PAD_IDX).to(device)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wylQOq8N8cqI",
        "colab_type": "text"
      },
      "source": [
        "### 損失関数とoptimizerとschedulerを作る"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gw34INS78cIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilWLfu8Z8MzW",
        "colab_type": "text"
      },
      "source": [
        "### 訓練用の関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR2R4Lqh8J7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(data_iterator, model, optimizer, scheduler, criterion):\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  train_loss = 0\n",
        "  train_acc = 0\n",
        "  for batch in data_iterator:\n",
        "    optimizer.zero_grad()\n",
        "    text, cls = batch.text, batch.label\n",
        "    output = model(text)\n",
        "    loss = criterion(output, cls)\n",
        "    train_loss += loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_acc += (output.argmax(1) == cls).sum().item()\n",
        "\n",
        "  scheduler.step()\n",
        "\n",
        "  data_len = len(data_iterator.dataset)\n",
        "  return train_loss / data_len, train_acc / data_len"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftuX8e1W8iRh",
        "colab_type": "text"
      },
      "source": [
        "### 評価用の関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGUnsJlq8Ue3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(data_iterator, model, criterion):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  loss = 0\n",
        "  acc = 0\n",
        "  for batch in data_iterator:\n",
        "    text, cls = batch.text, batch.label\n",
        "    with torch.no_grad():\n",
        "      output = model(text)\n",
        "      loss = criterion(output, cls)\n",
        "      loss += loss.item()\n",
        "      acc += (output.argmax(1) == cls).sum().item()\n",
        "\n",
        "  data_len = len(data_iterator.dataset)\n",
        "  return loss / data_len, acc / data_len"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8o_jDAg8osP",
        "colab_type": "text"
      },
      "source": [
        "## 07-03 分類器の訓練と開発セットでの評価"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJJFv4k-8mH1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "12fc9768-739e-4984-ff5b-f8990f40dc2b"
      },
      "source": [
        "N_EPOCHS = 20\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "  start_time = time.time()\n",
        "  train_loss, train_acc = train(train_iterator, model, optimizer, scheduler, criterion)\n",
        "  dev_loss, dev_acc = test(dev_iterator, model, criterion)\n",
        "\n",
        "  secs = int(time.time() - start_time)\n",
        "  mins = secs / 60\n",
        "  secs = secs % 60\n",
        "\n",
        "  print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
        "  print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
        "  print(f'\\tLoss: {dev_loss:.4f}(dev)\\t|\\tAcc: {dev_acc * 100:.1f}%(dev)')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  | time in 0 minutes, 5 seconds\n",
            "\tLoss: 0.0044(train)\t|\tAcc: 79.4%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.5%(dev)\n",
            "Epoch: 2  | time in 0 minutes, 5 seconds\n",
            "\tLoss: 0.0017(train)\t|\tAcc: 93.6%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 89.4%(dev)\n",
            "Epoch: 3  | time in 0 minutes, 5 seconds\n",
            "\tLoss: 0.0008(train)\t|\tAcc: 97.2%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.0%(dev)\n",
            "Epoch: 4  | time in 0 minutes, 5 seconds\n",
            "\tLoss: 0.0004(train)\t|\tAcc: 98.9%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.1%(dev)\n",
            "Epoch: 5  | time in 0 minutes, 5 seconds\n",
            "\tLoss: 0.0002(train)\t|\tAcc: 99.4%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 87.7%(dev)\n",
            "Epoch: 6  | time in 0 minutes, 5 seconds\n",
            "\tLoss: 0.0001(train)\t|\tAcc: 99.6%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.5%(dev)\n",
            "Epoch: 7  | time in 0 minutes, 5 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 99.9%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 87.3%(dev)\n",
            "Epoch: 8  | time in 0 minutes, 5 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 99.9%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 87.3%(dev)\n",
            "Epoch: 9  | time in 0 minutes, 5 seconds\n",
            "\tLoss: 0.0001(train)\t|\tAcc: 99.9%(train)\n",
            "\tLoss: 0.0004(dev)\t|\tAcc: 86.8%(dev)\n",
            "Epoch: 10  | time in 0 minutes, 5 seconds\n",
            "\tLoss: 0.0001(train)\t|\tAcc: 99.6%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 87.4%(dev)\n",
            "Epoch: 11  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 87.5%(dev)\n",
            "Epoch: 12  | time in 0 minutes, 5 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 87.4%(dev)\n",
            "Epoch: 13  | time in 0 minutes, 5 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0003(dev)\t|\tAcc: 87.5%(dev)\n",
            "Epoch: 14  | time in 0 minutes, 5 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0003(dev)\t|\tAcc: 87.4%(dev)\n",
            "Epoch: 15  | time in 0 minutes, 5 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0003(dev)\t|\tAcc: 87.4%(dev)\n",
            "Epoch: 16  | time in 0 minutes, 5 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0003(dev)\t|\tAcc: 87.4%(dev)\n",
            "Epoch: 17  | time in 0 minutes, 5 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0003(dev)\t|\tAcc: 87.4%(dev)\n",
            "Epoch: 18  | time in 0 minutes, 5 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0003(dev)\t|\tAcc: 87.4%(dev)\n",
            "Epoch: 19  | time in 0 minutes, 5 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0004(dev)\t|\tAcc: 87.3%(dev)\n",
            "Epoch: 20  | time in 0 minutes, 5 seconds\n",
            "\tLoss: 0.0000(train)\t|\tAcc: 100.0%(train)\n",
            "\tLoss: 0.0004(dev)\t|\tAcc: 87.3%(dev)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPux8PReWTXG",
        "colab_type": "text"
      },
      "source": [
        "## 07-04 再検討\n",
        "* 訓練データ上での分類精度が100%になってしまっている。明らかにオーバーフィッティング。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23jMgtmoWkty",
        "colab_type": "text"
      },
      "source": [
        "### ドロップアウトを使う\n",
        "* モデルのインスタンスを作るときにdropoutの確率を引数pで指定できるようにする。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khps3ZuBWntq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EmbedTextSentiment(nn.Module):\n",
        "  def __init__(self, embed_dim, num_class, vocab_size, padding_idx, p=0.0):\n",
        "    super(EmbedTextSentiment, self).__init__()\n",
        "    self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
        "    self.dropout = nn.Dropout(p=p)\n",
        "    self.fc1 = nn.Linear(embed_dim, 500)\n",
        "    self.fc2 = nn.Linear(500, 100)\n",
        "    self.fc3 = nn.Linear(100, num_class)\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "    initrange = 0.5\n",
        "    self.fc1.weight.data.uniform_(-initrange, initrange)\n",
        "    self.fc1.bias.data.zero_()\n",
        "    self.fc2.weight.data.uniform_(-initrange, initrange)\n",
        "    self.fc2.bias.data.zero_()\n",
        "    self.fc3.weight.data.uniform_(-initrange, initrange)\n",
        "    self.fc3.bias.data.zero_()\n",
        "\n",
        "  def forward(self, text):\n",
        "    x = self.dropout(self.embed(text))\n",
        "    x = x.mean(0)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVXbkt6qXxNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = EmbedTextSentiment(EMBED_DIM, NUM_CLASS, INPUT_DIM, padding_idx=PAD_IDX, p=0.5).to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXkBDXc6X1mp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5e85d255-e38b-4177-c986-b99a25077faa"
      },
      "source": [
        "N_EPOCHS = 20\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "  start_time = time.time()\n",
        "  train_loss, train_acc = train(train_iterator, model, optimizer, scheduler, criterion)\n",
        "  dev_loss, dev_acc = test(dev_iterator, model, criterion)\n",
        "\n",
        "  secs = int(time.time() - start_time)\n",
        "  mins = secs / 60\n",
        "  secs = secs % 60\n",
        "\n",
        "  print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
        "  print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
        "  print(f'\\tLoss: {dev_loss:.4f}(dev)\\t|\\tAcc: {dev_acc * 100:.1f}%(dev)')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0051(train)\t|\tAcc: 74.7%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 86.8%(dev)\n",
            "Epoch: 2  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0024(train)\t|\tAcc: 90.3%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.8%(dev)\n",
            "Epoch: 3  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0017(train)\t|\tAcc: 93.7%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.6%(dev)\n",
            "Epoch: 4  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0013(train)\t|\tAcc: 95.3%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.9%(dev)\n",
            "Epoch: 5  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0009(train)\t|\tAcc: 96.8%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.7%(dev)\n",
            "Epoch: 6  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0007(train)\t|\tAcc: 97.3%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.5%(dev)\n",
            "Epoch: 7  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0006(train)\t|\tAcc: 97.7%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.3%(dev)\n",
            "Epoch: 8  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0005(train)\t|\tAcc: 98.2%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.3%(dev)\n",
            "Epoch: 9  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0004(train)\t|\tAcc: 98.5%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.1%(dev)\n",
            "Epoch: 10  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0004(train)\t|\tAcc: 98.7%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.0%(dev)\n",
            "Epoch: 11  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0003(train)\t|\tAcc: 98.8%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.9%(dev)\n",
            "Epoch: 12  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0003(train)\t|\tAcc: 99.1%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.8%(dev)\n",
            "Epoch: 13  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0002(train)\t|\tAcc: 99.1%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.9%(dev)\n",
            "Epoch: 14  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0002(train)\t|\tAcc: 99.3%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 88.0%(dev)\n",
            "Epoch: 15  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0002(train)\t|\tAcc: 99.4%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 88.1%(dev)\n",
            "Epoch: 16  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0002(train)\t|\tAcc: 99.4%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 87.6%(dev)\n",
            "Epoch: 17  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0002(train)\t|\tAcc: 99.5%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.1%(dev)\n",
            "Epoch: 18  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0001(train)\t|\tAcc: 99.6%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 88.0%(dev)\n",
            "Epoch: 19  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0002(train)\t|\tAcc: 99.4%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 88.0%(dev)\n",
            "Epoch: 20  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0001(train)\t|\tAcc: 99.5%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 88.2%(dev)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu3Y-wjwb0po",
        "colab_type": "text"
      },
      "source": [
        "### L２正則化を使う\n",
        "* optimizerのweight_decayパラメータを0より大きな値にする。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmxEuSFJazCJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = EmbedTextSentiment(EMBED_DIM, NUM_CLASS, INPUT_DIM, padding_idx=PAD_IDX, p=0.5).to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0Zr2S7ga3J4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d57c78d5-c2e0-481e-dbb5-108804ff79ad"
      },
      "source": [
        "N_EPOCHS = 20\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "  start_time = time.time()\n",
        "  train_loss, train_acc = train(train_iterator, model, optimizer, scheduler, criterion)\n",
        "  dev_loss, dev_acc = test(dev_iterator, model, criterion)\n",
        "\n",
        "  secs = int(time.time() - start_time)\n",
        "  mins = secs / 60\n",
        "  secs = secs % 60\n",
        "\n",
        "  print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
        "  print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
        "  print(f'\\tLoss: {dev_loss:.4f}(dev)\\t|\\tAcc: {dev_acc * 100:.1f}%(dev)')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0067(train)\t|\tAcc: 60.9%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 69.2%(dev)\n",
            "Epoch: 2  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0052(train)\t|\tAcc: 74.4%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 80.1%(dev)\n",
            "Epoch: 3  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0045(train)\t|\tAcc: 80.2%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 80.1%(dev)\n",
            "Epoch: 4  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0040(train)\t|\tAcc: 83.0%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 85.4%(dev)\n",
            "Epoch: 5  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0039(train)\t|\tAcc: 83.3%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 84.2%(dev)\n",
            "Epoch: 6  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0036(train)\t|\tAcc: 85.2%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 84.7%(dev)\n",
            "Epoch: 7  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0035(train)\t|\tAcc: 85.8%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.1%(dev)\n",
            "Epoch: 8  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0034(train)\t|\tAcc: 86.4%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.1%(dev)\n",
            "Epoch: 9  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0032(train)\t|\tAcc: 87.2%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 85.9%(dev)\n",
            "Epoch: 10  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0031(train)\t|\tAcc: 87.2%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.6%(dev)\n",
            "Epoch: 11  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0030(train)\t|\tAcc: 88.4%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.2%(dev)\n",
            "Epoch: 12  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0029(train)\t|\tAcc: 88.8%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.4%(dev)\n",
            "Epoch: 13  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0028(train)\t|\tAcc: 89.6%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.2%(dev)\n",
            "Epoch: 14  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0027(train)\t|\tAcc: 89.6%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 85.9%(dev)\n",
            "Epoch: 15  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0027(train)\t|\tAcc: 89.8%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.7%(dev)\n",
            "Epoch: 16  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0025(train)\t|\tAcc: 90.9%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.9%(dev)\n",
            "Epoch: 17  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0025(train)\t|\tAcc: 91.2%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.5%(dev)\n",
            "Epoch: 18  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0024(train)\t|\tAcc: 91.2%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 89.0%(dev)\n",
            "Epoch: 19  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0023(train)\t|\tAcc: 91.9%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 89.0%(dev)\n",
            "Epoch: 20  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0022(train)\t|\tAcc: 92.2%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.9%(dev)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIHA64UTdmBj",
        "colab_type": "text"
      },
      "source": [
        "### early stopping\n",
        "* dev setでのaccuracyが4回連続で最高値を下回ったら訓練を終えることにする。\n",
        "* early stoppingの実現については、PyTorch Lightningを使う手もある。\n",
        " * https://pytorch-lightning.readthedocs.io/en/latest/early_stopping.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0zclQnVdlVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = EmbedTextSentiment(EMBED_DIM, NUM_CLASS, INPUT_DIM, padding_idx=PAD_IDX, p=0.5).to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3E_I5sRc3FF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "13411617-261f-4b31-ad1d-5681acf63708"
      },
      "source": [
        "patience = 4\n",
        "early_stop_count = 0\n",
        "best_dev_acc = 0.0\n",
        "dev_acc_threshold = 0.87\n",
        "\n",
        "N_EPOCHS = 50 # エポック数を増やしておく\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "  start_time = time.time()\n",
        "  train_loss, train_acc = train(train_iterator, model, optimizer, scheduler, criterion)\n",
        "  dev_loss, dev_acc = test(dev_iterator, model, criterion)\n",
        "\n",
        "  secs = int(time.time() - start_time)\n",
        "  mins = secs / 60\n",
        "  secs = secs % 60\n",
        "\n",
        "  print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
        "  print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
        "  print(f'\\tLoss: {dev_loss:.4f}(dev)\\t|\\tAcc: {dev_acc * 100:.1f}%(dev)')\n",
        "\n",
        "  # early stopping\n",
        "  if best_dev_acc <= dev_acc:\n",
        "    best_dev_acc = dev_acc\n",
        "    early_stop_count = 0\n",
        "  else:\n",
        "    early_stop_count += 1\n",
        "    if early_stop_count == patience:\n",
        "      break"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0065(train)\t|\tAcc: 62.6%(train)\n",
            "\tLoss: 0.0002(dev)\t|\tAcc: 73.1%(dev)\n",
            "Epoch: 2  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0050(train)\t|\tAcc: 76.6%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 82.8%(dev)\n",
            "Epoch: 3  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0045(train)\t|\tAcc: 79.1%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 84.0%(dev)\n",
            "Epoch: 4  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0040(train)\t|\tAcc: 82.2%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 83.3%(dev)\n",
            "Epoch: 5  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0038(train)\t|\tAcc: 84.2%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 86.4%(dev)\n",
            "Epoch: 6  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0036(train)\t|\tAcc: 84.9%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 86.7%(dev)\n",
            "Epoch: 7  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0034(train)\t|\tAcc: 86.0%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.0%(dev)\n",
            "Epoch: 8  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0034(train)\t|\tAcc: 85.9%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 86.4%(dev)\n",
            "Epoch: 9  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0032(train)\t|\tAcc: 87.5%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.7%(dev)\n",
            "Epoch: 10  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0031(train)\t|\tAcc: 87.5%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.1%(dev)\n",
            "Epoch: 11  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0030(train)\t|\tAcc: 88.0%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 87.2%(dev)\n",
            "Epoch: 12  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0029(train)\t|\tAcc: 88.5%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.4%(dev)\n",
            "Epoch: 13  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0028(train)\t|\tAcc: 89.4%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.6%(dev)\n",
            "Epoch: 14  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0027(train)\t|\tAcc: 90.0%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.8%(dev)\n",
            "Epoch: 15  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0026(train)\t|\tAcc: 90.5%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.8%(dev)\n",
            "Epoch: 16  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0025(train)\t|\tAcc: 90.9%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 89.2%(dev)\n",
            "Epoch: 17  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0025(train)\t|\tAcc: 90.7%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.5%(dev)\n",
            "Epoch: 18  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0024(train)\t|\tAcc: 91.0%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 89.3%(dev)\n",
            "Epoch: 19  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0023(train)\t|\tAcc: 91.7%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 89.2%(dev)\n",
            "Epoch: 20  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0023(train)\t|\tAcc: 92.2%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.8%(dev)\n",
            "Epoch: 21  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0022(train)\t|\tAcc: 92.4%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 88.8%(dev)\n",
            "Epoch: 22  | time in 0 minutes, 6 seconds\n",
            "\tLoss: 0.0021(train)\t|\tAcc: 92.7%(train)\n",
            "\tLoss: 0.0001(dev)\t|\tAcc: 89.2%(dev)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRvkncN09MKk",
        "colab_type": "text"
      },
      "source": [
        "## 07-05 テストセット上で評価\n",
        "* 見つけ出したベストな設定を使って、テストセット上での最終的な評価をおこなう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39XBHTCGhlUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_gHj4x38y8h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "32e8ba17-b947-42a2-f0b1-b8f7b6c4ef89"
      },
      "source": [
        "print('Checking the results of test dataset...')\n",
        "test_loss, test_acc = test(test_iterator, model, criterion)\n",
        "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking the results of test dataset...\n",
            "\tLoss: 0.0000(test)\t|\tAcc: 88.2%(test)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1M_VQ1xhcWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 37,
      "outputs": []
    }
  ]
}